{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyM7cSYFwQma3bLe42BWxC41"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n","|<h2>Section:</h2>|<h1>Embedding spaces<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Unembeddings (vectors to tokens)<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"cJ56VUGP2nQa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PDFppSRRopBP"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"iRAzf_6L1ZY0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Import GPT2 model and tokenizer, and get dimensions"],"metadata":{"id":"Gz9ZTkp51ZSy"}},{"cell_type":"code","source":["from transformers import GPT2Model,GPT2Tokenizer\n","\n","# pretrained GPT-2 model and tokenizer\n","gpt2\n","tokenizer"],"metadata":{"id":"B7PftXi1u5nh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# embeddings matrix\n","embeddings"],"metadata":{"id":"_DTHcukv6kO2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the size parameters in .config\n","gpt2"],"metadata":{"id":"R5GMjGhH1f5b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the properties we'll use later\n","print(f'Embedding dimensions:\n","print(f'Vocab size:\n","print(f'Size of embeddings matrix:"],"metadata":{"id":"59Xe6als6kiF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0i7XoGE91wKu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Real and random unembeddings"],"metadata":{"id":"c0gmlLRz6kVG"}},{"cell_type":"code","source":["# unembeddings matrix as the transpose of the (real) embeddings\n","\n","\n","# confirm that transposing matrix a copy\n","print('id of embeddings:  ',id(embeddings))\n","print('id of unembeddings:',id(unembeddings))"],"metadata":{"id":"bV0ppdoQ6kIk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# a random unembeddings matrix\n","unembeddingsRand =\n","\n","print(f'         Size of embeddings matrix: {embeddings.shape}')\n","print(f'Size of random unembeddings matrix: {unembeddingsRand.shape}')\n","print(f'  Size of real unembeddings matrix: {unembeddings.shape}')"],"metadata":{"id":"5rI2UTJE1zRG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"94k69u0w2MFm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: California embedding"],"metadata":{"id":"TToyw0eL2MAh"}},{"cell_type":"code","source":["# pick a word\n","seedword =\n","\n","# its token index\n","seed_idx = tokenizer.encode(seedword)\n","\n","# make sure it's one token\n","seed_idx"],"metadata":{"id":"NV2cynlbJepg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find its embedding vector\n","embed_vector =\n","\n","# plot it!\n","plt.figure(figsize=(10,3))\n","plt.scatter(,,s=30,c=abs(embed_vector),cmap='RdPu')\n","plt.gca().set(xlabel='Embedding dimension',ylabel='Embedding weight',xlim=[-3,gpt2.config.n_embd+2],\n","              title=f'Embedding (GPT2) of \"{tokenizer.decode(seed_idx)}\"')\n","plt.show()"],"metadata":{"id":"nWqt1Ks71Dpx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# project the embedding vector onto the unembedding matrix\n","dpRand =\n","\n","# next token is the maximum dot product (unscaled cosine similarity)!\n","nextTokenRand_idx =\n","nextTokenRand = tokenizer...\n","\n","# check the sizes\n","print('embed_vector  X  unembeddings  =  dotproducts')\n","print(f'  {embed_vector.shape}       {unembeddingsRand.shape}      {dpRand.shape}')"],"metadata":{"id":"DQRw7aceLdpV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# repeat for the real unembeddings matrix\n","dpReal =\n","nextTokenReal_idx\n","nextTokenReal ="],"metadata":{"id":"dg49K8OQ3ONM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('** Random unembeddings matrix:')\n","print(f'   \"{tokenizer.decode}\" has largest dot product with token \"{nextTokenRand}\"\\n')\n","\n","print('** Real unembeddings matrix:')\n"],"metadata":{"id":"oaRvqT0LJucr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot it!\n","_,axs = plt.subplots(1,2,figsize=(12,3))\n","axs[0].scatter(range(),,s=30,c=abs(dpRand),cmap='RdPu',alpha=.4)\n","axs[0].axvline(nextTokenRand_idx,linestyle='--',color='k',alpha=1/3)\n","axs[0].plot(nextTokenRand_idx,dpRand[0,nextTokenRand_idx],'gv')\n","axs[0].set(xlabel='Unembedding dimension',ylabel='Dot product',xlim=[-11,tokenizer.vocab_size+10],\n","              title=f'(Random) dot products with \"{tokenizer.decode(seed_idx)}\"')\n","\n","axs[1].scatter(,,s=30,c=abs(dpReal),cmap='RdPu',alpha=.4)\n","axs[1].axvline(nextTokenReal_idx,linestyle='--',color='k',alpha=1/3)\n","axs[1].plot(nextTokenReal_idx,dpReal[0,nextTokenReal_idx],'gv')\n","axs[1].set(xlabel='Unembedding dimension',ylabel='Dot product',xlim=[-11,tokenizer.vocab_size+10],\n","              title=f'(Real) dot products with \"{tokenizer.decode(seed_idx)}\"')\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"pLDQ-RwGJejA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"d3Bq4TwK6kYO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Find top-10 unembeddings"],"metadata":{"id":"c9PGqM69-8jf"}},{"cell_type":"code","source":["top10 = np.argsort(dpReal[0])[::-1][:10]\n","\n","for i in top10:\n"],"metadata":{"id":"UHQnvorp-8gn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tSQ7KDhm-8dw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Generate a token sequence"],"metadata":{"id":"5WdhW9Kh-8bI"}},{"cell_type":"code","source":["# sequence length\n","seq_len = 10\n","\n","# initial seed\n","nextword =\n","\n","# initializing a list that will contain the text\n","text = nextword\n","\n","\n","# loop to create the sequence\n","for i in\n","\n","  # step 1: tokenize\n","\n","\n","  # step 2: get embedding vector\n","\n","\n","  # step 3: project onto unembedding matrix (dot products)\n","\n","\n","  # step 4: find top10 projections\n","\n","\n","  # step 5: randomly pick one for next token\n","\n","\n","  # step 6: append the text\n","\n","\n","# print the final result!\n","print('Our very philosophically meaningful text:\\n',text)"],"metadata":{"id":"U6dH96C_-8YB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"itNx26ENxJOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Repeat with random unembeddings"],"metadata":{"id":"nmRfT3gJ3w4l"}},{"cell_type":"code","source":[],"metadata":{"id":"Z6Yal2r9xJLZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1umxbFabzMg3"},"execution_count":null,"outputs":[]}]}