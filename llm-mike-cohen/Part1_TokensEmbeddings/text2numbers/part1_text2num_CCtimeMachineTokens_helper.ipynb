{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n","|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Tokenizing The Time Machine<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"1pxg6ywI6EEw"}},{"cell_type":"code","source":[],"metadata":{"id":"7k510WhK6Dpe"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lwgCppyRKogB"},"outputs":[],"source":["# typical libraries...\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# for importing and working with texts\n","import requests\n","import re\n","import string\n","\n","# adjust matplotlib defaults to personal preferences\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"3xNZfqxm6m4I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Get and prepare the text"],"metadata":{"id":"fEFbCZ8FLEu8"}},{"cell_type":"code","source":["# get raw text from internet\n","text = requests.get('https://www.gutenberg.org/files/35/35-0.txt').text\n","\n","# character strings to replace with space\n","strings2replace = [ '\\r\\n\\r\\nâ\\x80\\x9c','â\\x80\\x9c','â\\x80\\x9d','\\r\\n','â\\x80\\x94','â\\x80\\x99','â\\x80\\x98','_', ]\n","\n","# use regular expression (re) to replace those strings with space\n","for str2match in strings2replace:\n","  text = re.compile().sub(\n","\n","# remove non-ASCII characters and numbers, and make lower-case\n","text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n","text = re.sub(r'\\d+','',text).make this lowercase"],"metadata":{"id":"EPRfkKgHLEsE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split into words that contain >1 character\n","words = re.split(fr'[{string.punctuation}\\s]+',text)\n","words = [item.strip() for item in words if item.strip()]\n","words = # save only the words with >1 chars\n","\n","# create the vocab / lexicon\n","vocab = # a sorted set\n","nWords =\n","nLex ="],"metadata":{"id":"YM_OBLi7zptg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create the encoder/decoding mapping dictionaries\n","word2idx = {w:i for i,w in enumerate()}\n","idx2word ="],"metadata":{"id":"Belr9eoCLTcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create encoder and decoder functions\n","def encoder(words,encode_dict):\n","\n","  # loop through the words and find their token in the vocab\n","  idxs = np.zeros(len(words),dtype=int)\n","  for i,w in enumerate(words):\n","    idxs\n","  return\n","\n","# and the decoder function\n","def decoder(idxs,decode_dict):\n","  return ' '.join(# the dictionary items"],"metadata":{"id":"2HYTeIawLEOk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7R9AB9nXY3Ru"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: A random walk through the Time Machine"],"metadata":{"id":"O59NxW2pY3U2"}},{"cell_type":"code","source":["# random tokens\n","randomTokens =\n","\n","# test with random token indices\n","print(f'Random tokens: \\n\\t{randomTokens}\\n')\n","print(f'Decoded text: \\n\\t{"],"metadata":{"id":"MpnKCGlNYyKn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A brief aside on Brownian noise\n","brownNoise = np.cumsum(np.random.choice([-1,1],3000))\n","\n","plt.figure(figsize=(10,3))\n","plt.plot(brownNoise,'k')\n","plt.gca().set(xlim=[0,len(brownNoise)],xlabel='\"Time\" (?)',ylabel='Signal amplitude',title='Brownian noise')\n","plt.show()"],"metadata":{"id":"dLNWiOh5V2zo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Brownian noise\n","brownNoise = np.cumsum( # a random sequence of +1 and -1\n","print(brownNoise)\n","\n","BrownianRandomTokens = brownNoise +\n","print(BrownianRandomTokens)\n","print('')\n","\n","# test with random token indices\n","print(f'Brownian random tokens\n","print(f'Decoded text"],"metadata":{"id":"cIpqrE5rKGbe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gEzmF7q5LXeD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Distribution of word lengths"],"metadata":{"id":"NXFIvKbFLXa1"}},{"cell_type":"code","source":["# loop through the words and count the characters per word\n","numChars =\n","for i,w in enumerate(words):\n","  numChars[i] =\n","\n","# now count the number of words with those characters\n","charCounts =\n","for i in\n","  charCounts[i] = np.sum(numChars==i)\n","\n","\n","# and plot\n","_,axs = plt.subplots(2,1,figsize=(10,7))\n","axs[0].scatter(range(nWords),numChars,marker='.',s=10,c=np.linspace(.1,.9,len(numChars)),alpha=.4)\n","axs[0].set(yticks=range(1,int(np.max(numChars))),xlabel='Token index',xlim=[-15,nWords+15],\n","           ylabel='Number of characters',title='Character count by token index')\n","\n","axs[1].bar(range(len(charCounts)),charCounts,edgecolor='k',color=[.9,.7,.9])\n","axs[1].set(xticks=range(1,len(charCounts)),xlim=[0,len(charCounts)],xlabel='Number of characters',\n","           ylabel='Token count',title='Histogram of character count frequencies')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"YFZwHX25MQuD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Gcbcj7JsLXX6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Encode a novel sentence"],"metadata":{"id":"SJacP80VRwbV"}},{"cell_type":"code","source":["# the text to decode\n","sentence = 'The space aliens came to Earth to steal watermelons and staplers.'\n","\n","# preprocess (remove punctuation, make lower-case, split into words)\n","words_new = re.split(f'[,.\\s]+',\n","\n","# remove empty items\n","words_new = [item.strip() for item in words_new if item.strip()]\n","words_new"],"metadata":{"id":"unIc2KwtRaGt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenize (uh oh...)\n","encoder(words_new,word2idx)"],"metadata":{"id":"nVrNpUxeVQgi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pkZSVYVbgJE6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Create a new encoder"],"metadata":{"id":"GzV3-uvcgJCh"}},{"cell_type":"code","source":["# need to update the vocab\n","word2idx_new = word2idx.copy()\n","idx2word_new = idx2word.copy()\n","\n","# add an entry for unknown words\n","word2idx_new['<|unk|>'] =\n","idx2word_new... = '<|unk|>'"],"metadata":{"id":"z_8CpZRLRaEE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# need a new encoder function\n","def encoder_new(words,encode_dict):\n","\n","  # initialize a vector of numerical indices\n","  idxs = np.zeros(len(words),dtype=int)\n","\n","  # loop through the words and find their token in the vocab\n","  for i,w in enumerate(\n","    if w in encode_dict:\n","\n","    else:\n","\n","\n","  # return the results!\n","  return idxs"],"metadata":{"id":"1u6Vb4_SdDiN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# try again\n","tokenidx = encoder_new(words_new,word2idx_new)\n","tokenidx"],"metadata":{"id":"H6TZbghCRaBU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# need a new decoder function?\n","decoder(tokenidx,idx2word_new)"],"metadata":{"id":"dKDT_rlPRZ8S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AFEeR0URLXSd"},"execution_count":null,"outputs":[]}]}