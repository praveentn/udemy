{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n",
        "|-|:-:|\n",
        "|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n",
        "|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n",
        "|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Tokenizing The Time Machine<b></h1>|\n",
        "\n",
        "<br>\n",
        "\n",
        "<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n",
        "<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n",
        "<i>Using the code without the course may lead to confusion or errors.</i>"
      ],
      "metadata": {
        "id": "1pxg6ywI6EEw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7k510WhK6Dpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwgCppyRKogB"
      },
      "outputs": [],
      "source": [
        "# typical libraries...\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for importing and working with texts\n",
        "import requests\n",
        "import re\n",
        "import string\n",
        "\n",
        "# adjust matplotlib defaults to personal preferences\n",
        "import matplotlib_inline.backend_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3xNZfqxm6m4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: Get and prepare the text"
      ],
      "metadata": {
        "id": "fEFbCZ8FLEu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get raw text from internet\n",
        "text = requests.get('https://www.gutenberg.org/files/35/35-0.txt').text\n",
        "\n",
        "# character strings to replace with space\n",
        "strings2replace = [ '\\r\\n\\r\\nâ\\x80\\x9c','â\\x80\\x9c','â\\x80\\x9d','\\r\\n','â\\x80\\x94','â\\x80\\x99','â\\x80\\x98','_', ]\n",
        "\n",
        "# use regular expression (re) to replace those strings with space\n",
        "for str2match in strings2replace:\n",
        "  text = re.compile(r'%s'%str2match).sub(' ',text)\n",
        "\n",
        "# remove non-ASCII characters and numbers, and make lower-case\n",
        "text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "text = re.sub(r'\\d+','',text).lower()"
      ],
      "metadata": {
        "id": "EPRfkKgHLEsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split into words that contain >1 character\n",
        "words = re.split(fr'[{string.punctuation}\\s]+',text)\n",
        "words = [item.strip() for item in words if item.strip()]\n",
        "words = [item for item in words if len(item)>1]\n",
        "\n",
        "# create the vocab / lexicon\n",
        "vocab = sorted(set(words))\n",
        "nWords = len(words)\n",
        "nLex = len(vocab)"
      ],
      "metadata": {
        "id": "YM_OBLi7zptg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the encoder/decoding mapping dictionaries\n",
        "word2idx = {w:i for i,w in enumerate(vocab)}\n",
        "idx2word = {i:w for i,w in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "Belr9eoCLTcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create encoder and decoder functions\n",
        "def encoder(words,encode_dict):\n",
        "\n",
        "  # loop through the words and find their token in the vocab\n",
        "  idxs = np.zeros(len(words),dtype=int)\n",
        "  for i,w in enumerate(words):\n",
        "    idxs[i] = encode_dict[w]\n",
        "  return idxs\n",
        "\n",
        "# and the decoder function\n",
        "def decoder(idxs,decode_dict):\n",
        "  return ' '.join([decode_dict[i] for i in idxs])"
      ],
      "metadata": {
        "id": "2HYTeIawLEOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7R9AB9nXY3Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2: A random walk through the Time Machine"
      ],
      "metadata": {
        "id": "O59NxW2pY3U2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# random tokens\n",
        "randomTokens = np.random.randint(0,len(vocab),10)\n",
        "\n",
        "# test with random token indices\n",
        "print(f'Random tokens: \\n\\t{randomTokens}\\n')\n",
        "print(f'Decoded text: \\n\\t{decoder(randomTokens,idx2word)}')"
      ],
      "metadata": {
        "id": "MpnKCGlNYyKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A brief aside on Brownian noise\n",
        "brownNoise = np.cumsum(np.random.choice([-1,1],3000))\n",
        "\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.plot(brownNoise,'k')\n",
        "plt.gca().set(xlim=[0,len(brownNoise)],xlabel='\"Time\" (?)',ylabel='Signal amplitude',title='Brownian noise')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dLNWiOh5V2zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Brownian noise\n",
        "brownNoise = np.cumsum(np.random.choice([-1,1],30))\n",
        "print(brownNoise)\n",
        "\n",
        "BrownianRandomTokens = brownNoise + np.random.choice(nLex,1)\n",
        "print(BrownianRandomTokens)\n",
        "print('')\n",
        "\n",
        "# test with random token indices\n",
        "print(f'Brownian random tokens: \\n\\t{BrownianRandomTokens}\\n')\n",
        "print(f'Decoded text: \\n\\t{decoder(BrownianRandomTokens,idx2word)}')"
      ],
      "metadata": {
        "id": "cIpqrE5rKGbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gEzmF7q5LXeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3: Distribution of word lengths"
      ],
      "metadata": {
        "id": "NXFIvKbFLXa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loop through the words and count the characters per word\n",
        "numChars = np.zeros(nWords,dtype=int)\n",
        "for i,w in enumerate(words):\n",
        "  numChars[i] = len(w)\n",
        "\n",
        "# now count the number of words with those characters\n",
        "charCounts = np.zeros(np.max(numChars)+1, dtype=int)\n",
        "for i in range(len(charCounts)):\n",
        "  charCounts[i] = np.sum(numChars==i)\n",
        "\n",
        "\n",
        "# and plot\n",
        "_,axs = plt.subplots(2,1,figsize=(10,7))\n",
        "axs[0].scatter(range(nWords),numChars,marker='.',s=10,c=np.linspace(.1,.9,len(numChars)),alpha=.4)\n",
        "axs[0].set(yticks=range(1,int(np.max(numChars))),xlabel='Token index',xlim=[-15,nWords+15],\n",
        "           ylabel='Number of characters',title='Character count by token index')\n",
        "\n",
        "axs[1].bar(range(len(charCounts)),charCounts,edgecolor='k',color=[.9,.7,.9])\n",
        "axs[1].set(xticks=range(1,len(charCounts)),xlim=[0,len(charCounts)],xlabel='Number of characters',\n",
        "           ylabel='Token count',title='Histogram of character count frequencies')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YFZwHX25MQuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gcbcj7JsLXX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4: Encode a novel sentence"
      ],
      "metadata": {
        "id": "SJacP80VRwbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the text to decode\n",
        "sentence = 'The space aliens came to Earth to steal watermelons and staplers.'\n",
        "\n",
        "# preprocess (remove punctuation, make lower-case, split into words)\n",
        "words_new = re.split(fr'[,.\\s]+',sentence.lower())\n",
        "\n",
        "# remove empty items\n",
        "words_new = [item.strip() for item in words_new if item.strip()]\n",
        "words_new"
      ],
      "metadata": {
        "id": "unIc2KwtRaGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize (uh oh...)\n",
        "encoder(words_new,word2idx)"
      ],
      "metadata": {
        "id": "nVrNpUxeVQgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pkZSVYVbgJE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5: Create a new encoder"
      ],
      "metadata": {
        "id": "GzV3-uvcgJCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# need to update the vocab\n",
        "word2idx_new = word2idx.copy()\n",
        "idx2word_new = idx2word.copy()\n",
        "\n",
        "# add an entry for unknown words\n",
        "word2idx_new['<|unk|>'] = len(word2idx)+1\n",
        "idx2word_new[len(idx2word)+1] = '<|unk|>'"
      ],
      "metadata": {
        "id": "z_8CpZRLRaEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# need a new encoder function\n",
        "def encoder_new(words,encode_dict):\n",
        "\n",
        "  # initialize a vector of numerical indices\n",
        "  idxs = np.zeros(len(words),dtype=int)\n",
        "\n",
        "  # loop through the words and find their token in the vocab\n",
        "  for i,w in enumerate(words):\n",
        "    if w in encode_dict:\n",
        "      idxs[i] = encode_dict[w]\n",
        "    else:\n",
        "      idxs[i] = encode_dict['<|unk|>']\n",
        "\n",
        "  # return the results!\n",
        "  return idxs\n",
        "\n",
        "  # note: could use list-comp:\n",
        "  #return np.array([ encode_dict[w] if w in encode_dict else encode_dict['<|unk|>'] for w in words ])"
      ],
      "metadata": {
        "id": "1u6Vb4_SdDiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try again\n",
        "tokenidx = encoder_new(words_new,word2idx_new)\n",
        "tokenidx"
      ],
      "metadata": {
        "id": "H6TZbghCRaBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# need a new decoder function?\n",
        "decoder(tokenidx,idx2word_new)"
      ],
      "metadata": {
        "id": "dKDT_rlPRZ8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AFEeR0URLXSd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}