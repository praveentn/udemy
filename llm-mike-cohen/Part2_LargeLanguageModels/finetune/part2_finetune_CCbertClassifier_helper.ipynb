{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyO4zYQyzjwBBaVAiHRZfmd6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Fine-tune pretrained models<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: IMDB Sentiment analysis using BERT<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"qyGsqU_wT0qV"}},{"cell_type":"code","source":["# run this code, then restart the python session (and then comment it out)\n","# !pip install -U datasets huggingface_hub fsspec"],"metadata":{"id":"E0SMT20eAmNm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# typical python libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","# pytorch libraries\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","\n","# huggingface libraries\n","from transformers import BertModel, BertTokenizer\n","from datasets import load_dataset, DatasetDict"],"metadata":{"id":"-mhNmp9CT17A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","bert = BertModel.from_pretrained('bert-base-uncased').to(device)"],"metadata":{"id":"zGSG7ILmT13L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dMKtr8CWeK72"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Import and process the dataset"],"metadata":{"id":"KSEbXYTweK5M"}},{"cell_type":"code","source":["# load the IMDB dataset (from HF)\n","dataset = load_dataset('imdb')\n","\n","# reduce the size (overwriting the variable!)\n","dataset = DatasetDict({split:dataset[split].select(range(5_000,20_000)) for split in ['train','test']})"],"metadata":{"id":"wLWXY0dnb4AA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,3))\n","plt.plot(,'m.',markersize=1,alpha=.2)\n","\n","plt.gca().set(\n","plt.show()"],"metadata":{"id":"vA77mq5YeVhR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define a tokenization function that processes each data sample\n","def tokenize_function(one_sample):\n","  return tokenizer(\n","    one_sample['text'],\n","    max_length = 512,         # Maximum sequence length\n","    padding    = 'max_length',# Pad sequences to the maximum length\n","    truncation = True)        # Truncate sequences longer than max_length\n","\n","\n","# apply the tokenization function to the dataset (batched for efficiency)\n","tokenized_dataset = dataset.map(tokenize_function, batched=True)\n","\n","# remove text pair\n","tokenized_dataset = tokenized_dataset.remove_columns\n","\n","# change format to pytorch tensors\n","tokenized_dataset.set_format(  , columns=['input_ids', 'attention_mask', 'label'])\n","\n","# create DataLoaders for training and testing\n","train_dataloader = DataLoader(tokenized_dataset['train'], shuffle=True, batch_size=32)\n","test_dataloader  ="],"metadata":{"id":"kJh6cuaYePG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next(iter(train_dataloader))"],"metadata":{"id":"wFui0MzSErj8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DglXfXWcb36U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Create and precision-freeze a model"],"metadata":{"id":"mQ0JGbZ3T1pm"}},{"cell_type":"code","source":["class BertForBinaryClassification(nn.Module):\n","  def __init__(self, num_labels=2):\n","    super(BertForBinaryClassification, self).__init__()\n","\n","    # Load the pre-trained BERT model.\n","    self.bert =\n","\n","    # classification head that converts the 768-d pooled output into 2 final outputs.\n","    self.classifier =\n","    self.dropout = nn.Dropout(self.bert.embeddings.dropout.p) # 10%\n","\n","    # initialize the weights and biases\n","    nn.init.xavier_uniform_(self.classifier....)\n","    nn.init.zeros_()\n","\n","\n","  def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n","\n","    # forward pass through the downloaded (pretrained) BERT\n","    outputs = self.bert(\n","      input_ids      = input_ids,\n","      attention_mask = attention_mask,\n","      token_type_ids = token_type_ids)\n","\n","    # extract the pooled output and apply dropout\n","    pooled_output = self.dropout( outputs.pooler_output )\n","\n","    # final push through the classification layer.\n","    logits =\n","    return logits"],"metadata":{"id":"05j-M4MRb4Iy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create an instance of the model and test it:\n","model = BertForBinaryClassification().to(device)\n","model"],"metadata":{"id":"SLb0HwWLciY2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VRcn-ubxOZPp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## freeze the attention weights\n","trainParamsCount = 0\n","frozenParamsCount = 0\n","\n","for name,param in model.named_parameters():\n","  if ('attention' in name) or ('embeddings' in name):\n","\n","    print(f'--- Layer {name} is frozen (.requires_grad = {param.requires_grad}).')\n","\n","  else:\n","    param.requires_grad = True # insurance :P\n","\n","    print(f'+++ Layer {name} is trainable (.requires_grad = {param.requires_grad}).')\n","\n","print(f'\\n\\nThere are {:,} ({):.2f}%) frozen weights,')\n","print(f'      and {:,} ({):.2f}%) trainable weights.')"],"metadata":{"id":"1pvNPedCPB2I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"83CMbsEyA3qH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Fine-tune the model"],"metadata":{"id":"Ftzlsxq_b33g"}},{"cell_type":"code","source":["# training hyperparameters\n","num_samples = 300\n","\n","# optimizer and loss function\n","optimizer =\n","loss_fun = nn."],"metadata":{"id":"LrR83zjvoy-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize performance metrices\n","train_losses = np.zeros(num_samples)\n","train_accuracy = np.zeros(num_samples)\n","test_losses = np.zeros(num_samples)\n","test_accuracy = np.zeros(num_samples)\n","\n","\n","\n","## loop over data samples\n","for sampli in range(num_samples):\n","\n","  # get a batch of data\n","  batch = next(iter(\n","\n","  # and move it to the GPU\n","  tokenz  = batch['input_ids']\n","  att_msk = batch\n","  labels  = batch\n","\n","  # clear the previous gradients\n","  optimizer.zero_grad()\n","\n","  # forward pass and get model predictions\n","  logits = model()\n","  predLabels = torch.argmax(, dim=)\n","\n","  # calculate and store loss + average accuracy\n","  loss = loss_fun(, )\n","  train_losses[sampli] = loss.item()\n","  train_accuracy[sampli] =\n","\n","  # backward pass\n","  loss.backward()\n","\n","  # update the weights and the learning rate\n","  optimizer.step()\n","\n","  # test the model and report losses every k samples\n","  if sampli%10 == 0:\n","\n","    # evaluation using the test set\n","    model.eval()\n","    with torch.no_grad():\n","\n","      # get a batch of data and move it to the GPU\n","      batch   =\n","      tokenz  =\n","      att_msk =\n","      labels  =\n","\n","      # forward pass and get model predictions\n","      logits = model(, attention_mask=)\n","      predLabels =\n","\n","      # calculate and store loss + accuracy\n","      loss = loss_fun()\n","      test_losses[sampli] = loss.item()\n","      test_accuracy[sampli] =\n","\n","      # report the results\n","      print(f'Sample {:4}/{}, losses (train/test): {:.2f}/{]:.2f}, accuracy: {:.2f}/{:.2f}')\n","\n","      # put the model back into train mode\n","      model.train()"],"metadata":{"id":"v5m3eWnynMeb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,3.5))\n","\n","# plot the losses\n","\n","\n","# plot the prediction accuracy\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"dKn24FCtnMbd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"B9VzgEzjnMYZ"},"execution_count":null,"outputs":[]}]}