{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNk/UY4V/fBa3YYyk0xOubo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dulm_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Fine-tune pretrained models<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Gulliver's learning rates<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dulm_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dulm_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"wzuj7gIJkgWr"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from transformers import AutoModelForCausalLM,GPT2Tokenizer\n","import requests\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"],"metadata":{"id":"aoocnKDi-2RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load GPT-2 tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","# hyperparameters\n","seq_len    = 256 # max sequence length\n","batch_size =  16\n","\n","# use GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"bHKfbxSx-B5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E5Kk_k9Noaio"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Functions to train and evaluate the model"],"metadata":{"id":"BLpOjjndmitp"}},{"cell_type":"code","source":["# tokenize Gulliver's travels\n","text = requests.get('https://www.gutenberg.org/cache/epub/829/pg829.txt').text\n","gtTokens = tokenizer.encode(text,return_tensors='pt')[0]\n","\n","# find the most frequent 100 tokens\n","uniq,counts = np.unique(gtTokens,return_counts=True)\n","freqidx = np.argsort(counts)[::-1]\n","top100 = uniq[freqidx[:100]]"],"metadata":{"id":"lP_txTZrkzLn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EbExPkfLV_fN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def countFreqTokens(model):\n","\n","  # random starting tokens\n","  numreps =  10 # number of random repetitions\n","  numtoks = 100 # output length\n","  randstarts = torch.randint(tokenizer.vocab_size,(numreps,1)).to(device)\n","\n","  # generate some data\n","  out = model.generate(\n","    randstarts,\n","    max_length = numtoks+1,\n","    min_length = numtoks+1,\n","    do_sample  = True,\n","    bad_words_ids = [tokenizer.encode(tokenizer.eos_token)],\n","    pad_token_id = tokenizer.encode(tokenizer.eos_token)[0]\n","  ).cpu()\n","\n","  # return proportion\n","  return np.mean(100*np.isin(out[:,1:],top100).flatten())"],"metadata":{"id":"n-qKxxzgnpOa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def trainTheModel(lr,num_samples):\n","\n","  # download a fresh copy of the model\n","  gpt2 = AutoModelForCausalLM.from_pretrained('gpt2').to(device)\n","\n","  # pre-train evaluation\n","  pretrainEval = countFreqTokens(gpt2)\n","\n","  # create the optimizer functions\n","  optimizer = torch.optim.AdamW(gpt2.parameters(), lr=lr, weight_decay=.01)\n","\n","  # initialize losses\n","  train_loss = np.zeros(num_samples)\n","\n","\n","  ### now for the training\n","  for sampli in range(num_samples):\n","\n","    # get a batch of data\n","    ix = torch.randint(len(gtTokens)-seq_len,size=(batch_size,))\n","    X  = gtTokens[ix[:,None] + torch.arange(seq_len)].to(device)\n","\n","    # forward pass (Hugging Face shifts X internally to get y)\n","    gpt2.zero_grad()\n","    outputs = gpt2(X,labels=X)\n","    loss = outputs.loss\n","\n","    # backprop\n","    loss.backward()\n","    optimizer.step()\n","    train_loss[sampli] = loss.item()\n","\n","\n","\n","  # post-train evaluation\n","  psttrainEval = countFreqTokens(gpt2)\n","\n","  return train_loss,pretrainEval,psttrainEval"],"metadata":{"id":"qpcC47LUy7w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WUYuqpThHZL5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Fine-tune the model with different learning rates"],"metadata":{"id":"JNxecnAYpNsV"}},{"cell_type":"code","source":["learningRates = [ 1e-4,1e-5,1e-6 ]\n","training_samples = 800\n","\n","evalsPcts = np.zeros((3,2))\n","losses = []\n","\n","\n","for idx,lr in enumerate(learningRates):\n","\n","  # train a fresh model and get the results\n","  train_loss,pretrainEval,psttrainEval = trainTheModel(lr,training_samples)\n","\n","  # store the results\n","  evalsPcts[idx,0] = pretrainEval\n","  evalsPcts[idx,1] = psttrainEval\n","  losses.append(train_loss)"],"metadata":{"id":"sb8bqSNdpSTH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nSqWEd29pNmn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Compare the evaluations"],"metadata":{"id":"oMKA4BKnky1M"}},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","colors = [ [.7,.7,.9],[.7,.9,.7],[.9,.7,.7] ]\n","shapes = 'so^'\n","\n","# plot the losses\n","for i in range(3):\n","\n","  # plot the losses\n","  axs[0].plot(range(0,training_samples,7),losses[i][::7],f'k{shapes[i]}',markerfacecolor=colors[i],\n","              alpha=.7,markersize=8,label=f'lr = {learningRates[i]}')\n","\n","  # plot the percent of common GT tokens\n","  axs[1].bar([i-.2,i+.2],evalsPcts[i,:],width=.4,edgecolor='k',\n","             facecolor=colors[i],label=f'lr = {learningRates[i]}')\n","\n","\n","\n","axs[0].set(xlabel='Training sample',ylabel='Train loss',title='Losses')\n","axs[0].legend()\n","\n","axs[1].set(xlabel='Training sample',xticks=[-.2,.2,.8,1.2,1.8,2.2],\n","           xticklabels=['pre','post','pre','post','pre','post'],\n","           title='Percent GT tokens generated',ylabel='% common GT tokens',ylim=[30,65])\n","axs[1].legend()\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"5XdjwacoHZOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1wZg_eplZBDV"},"execution_count":null,"outputs":[]}]}