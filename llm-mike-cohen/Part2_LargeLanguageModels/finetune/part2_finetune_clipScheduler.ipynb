{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPIl80Mnwy7+verCJxnq8hT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Fine-tune pretrained models<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Gradient clipping and learning rate scheduler<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"RJUfLWLdAwwm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VLXVvvBGzYIf"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from transformers import get_cosine_schedule_with_warmup,get_linear_schedule_with_warmup\n","\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"5HqQbj4hze10"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Simple demo of gradient clipping"],"metadata":{"id":"t_FulJy9zexj"}},{"cell_type":"code","source":["# tensor with gradients (like a weights matrix)\n","w = torch.tensor([[-1, 3.3, 2, -5, 3, -2, -4, -5, 1.5]], requires_grad=True)\n","\n","# loss is sum of squares (L2)\n","loss = (w**2).sum()\n","\n","# backprop\n","loss.backward()\n","\n","# print the gradients and their norm before clipping\n","print('BEFORE CLIPPING:')\n","print(f' Gradient vals: {w.grad[0].tolist()}')\n","print(f' Gradient norm: {torch.norm(w.grad):.3f}')\n","\n","# apply gradient clipping\n","preClipVals = w.grad[0].detach() + 0\n","nn.utils.clip_grad_norm_([w], max_norm=1)\n","\n","# print the gradients again\n","print('\\nAFTER CLIPPING:')\n","print(f' Gradient vals: {w.grad[0].tolist()}')\n","print(f' Gradient norm: {torch.norm(w.grad):.3f}')"],"metadata":{"id":"laCzKFNZ_3wd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LB8jT4W0MfYe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# their correlation\n","r = torch.corrcoef(torch.cat((preClipVals.unsqueeze(0),w.grad.detach()),dim=0))\n","\n","plt.plot(preClipVals,w.grad.detach().squeeze(),'ko',markersize=10,markerfacecolor=[.9,.7,.9])\n","plt.gca().set(xlabel='Pre-clipped values',ylabel='Clipped values',\n","              title=f'r = {r[0,1]:.2f}')\n","plt.grid(color=[.9,.9,.9])\n","plt.show()"],"metadata":{"id":"cagLlDMf_3tU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-woqLSFY_Jrg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Gradient clipping demo during learning"],"metadata":{"id":"-k7zjbm4_Jov"}},{"cell_type":"code","source":["# training time\n","training_steps = 80\n","\n","# define a weight matrix (requires_grad=True to track gradients)\n","w = torch.tensor([[-4,.2]], requires_grad=True)\n","\n","# target category\n","target = torch.tensor([0])\n","\n","# optimizer and loss function\n","loss_function = nn.NLLLoss()\n","optimizer = torch.optim.SGD([w],lr=.03)\n","\n","# training iterations\n","allWeights = torch.zeros((training_steps+1,2))\n","allWeights[0,:] = w.detach()\n","grad_norms = np.zeros((training_steps,2))\n","\n","# training loop\n","for i in range(training_steps):\n","\n","  # reset gradients\n","  optimizer.zero_grad()\n","\n","  # model outputs (simulating a full model forward pass ;)  )\n","  modeloutput = F.log_softmax(w,dim=1)\n","\n","  # loss\n","  loss = loss_function(modeloutput,target)\n","\n","  # gradient descent\n","  loss.backward()  # calculate gradient of loss wrt w\n","\n","  # store the gradient norm\n","  grad_norms[i,0] = torch.norm(w.grad)\n","\n","  # engage backprop (uncomment the clipping line after visualizing)\n","  # nn.utils.clip_grad_norm_([w], max_norm=1)\n","  optimizer.step() # adjust w\n","\n","  # clip the gradient and recalculate its norm\n","  nn.utils.clip_grad_norm_([w], max_norm=1)\n","  grad_norms[i,1] = torch.norm(w.grad)\n","\n","  # store the new weights\n","  allWeights[i+1,:] = w.detach()"],"metadata":{"id":"MjRMc6o3zet5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# let's see the weights!\n","_,axs = plt.subplots(2,1,figsize=(9,7))\n","axs[0].plot(allWeights[:,0],'ks',markerfacecolor=[.9,.7,.7],markersize=6,label='Weight 0 (target)')\n","axs[0].plot(allWeights[:,1],'ko',markerfacecolor=[.7,.7,.9],markersize=6,label='Weight 1 (non-target)')\n","axs[0].set(xlabel='Training epochs',ylabel='Weight value',xlim=[-3,training_steps+3])\n","axs[0].legend()\n","\n","axs[1].plot(grad_norms[:,0],'ko',markerfacecolor=[.7,.9,.7],markersize=6,alpha=.7,label='Pre-clipping')\n","axs[1].plot(grad_norms[:,1],'rs',markerfacecolor=[.9,.9,.7],markersize=6,alpha=.7,label='Post-clipping')\n","axs[1].set(xlabel='Training epoch',ylabel='Gradient norm',xlim=[-3,training_steps+3])\n","axs[1].legend()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"xMcTigwSzeqb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QmJ_NImS_H9m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OYAh-Zy1zenS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Simple demo of optimizer scheduler"],"metadata":{"id":"xaVrk8tVzekH"}},{"cell_type":"code","source":["# training steps\n","training_steps = 200\n","\n","# create a \"model\" and an optimizer\n","model = nn.Linear(10,10)\n","optimizer = torch.optim.AdamW(model.parameters(),lr=3e-5)\n","\n","# learning rate scheduler\n","scheduler = get_cosine_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps = int(training_steps*.1), # first 10% of training is warm-up\n","    num_training_steps = training_steps,\n","    num_cycles = .5 # in cycles over the entire training course\n",")"],"metadata":{"id":"h5BFE4AYzehG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dir(scheduler)"],"metadata":{"id":"Z2Hbfqf1zeeM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# quick test to see the learning rates\n","lrs = np.zeros(training_steps)\n","for i in range(training_steps):\n","  optimizer.step() # update the optimizer\n","  scheduler.step() # run the scheduler\n","  lrs[i] = scheduler.get_last_lr()[0] # get the actual learning rate\n","\n","# plot!\n","plt.figure(figsize=(10,3))\n","plt.plot(lrs,'ko',markersize=5,markerfacecolor=[.7,.7,.9],alpha=.3)\n","\n","plt.gca().set(xlabel='Training epoch',ylabel='Learning rate')\n","plt.show()"],"metadata":{"id":"U1BGK0LDzebG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4fJMRkcF1_SF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Demo of scheduler in a learning model"],"metadata":{"id":"_i74L94r1_Ph"}},{"cell_type":"code","source":["# redefine training time\n","training_steps = 5000\n","\n","# define a weight matrix (requires_grad=True to track gradients)\n","w = torch.tensor([[-1,.2]], requires_grad=True)\n","\n","# target category\n","target = torch.tensor([0])\n","\n","# optimizer and loss function\n","loss_function = nn.NLLLoss()\n","optimizer = torch.optim.AdamW([w],lr=.0005)\n","\n","### --- pick one of these two --- ###\n","scheduler = get_cosine_schedule_with_warmup(optimizer,\n","    num_warmup_steps = int(training_steps*.2), # first 20% of training is warm-up\n","    num_training_steps = training_steps,\n","    num_cycles = 2) # note the increase in cycles compared to previous\n","\n","# scheduler = get_linear_schedule_with_warmup(optimizer,\n","#     num_warmup_steps = int(training_steps*.2), # first 20% of training is warm-up\n","#     num_training_steps = training_steps-int(training_steps*.15))  # last 15% of training has no learning, but change the - to +\n","### ---------------------------- ###\n","\n","# training iterations\n","allWeights = torch.zeros((training_steps+1,2))\n","allWeights[0,:] = w.detach()\n","lrs = torch.zeros(training_steps)\n","\n","# training loop\n","for i in range(training_steps):\n","\n","  # reset gradients\n","  optimizer.zero_grad()\n","\n","  # model outputs (simulating a full model forward pass ;)  )\n","  modeloutput = F.log_softmax(w,dim=1)\n","\n","  # loss\n","  loss = loss_function(modeloutput,target)\n","\n","  # gradient descent\n","  loss.backward()  # calculate gradient of loss wrt w\n","  optimizer.step() # adjust w\n","  scheduler.step()\n","  lrs[i] = scheduler.get_last_lr()[0]\n","\n","  # store the new weights\n","  allWeights[i+1,:] = w.detach()"],"metadata":{"id":"I9yHtB0D31_C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# let's see the weights!\n","_,axs = plt.subplots(2,1,figsize=(9,7))\n","axs[0].plot(allWeights[:,0],linewidth=2,label='Weight 0 (target)')\n","axs[0].plot(allWeights[:,1],linewidth=2,label='Weight 1 (non-target)')\n","axs[0].set(xlabel='Training epochs',ylabel='Weight value',xlim=[-3,training_steps+3])\n","axs[0].legend()\n","\n","axs[1].plot(lrs,'k',linewidth=2)\n","axs[1].set(xlabel='Training epoch',ylabel='Learning rate',xlim=[-3,training_steps+3])\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"OCArCEl63177"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UDrHtqLD3145"},"execution_count":null,"outputs":[]}]}