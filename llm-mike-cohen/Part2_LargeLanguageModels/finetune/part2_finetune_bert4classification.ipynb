{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyM/HHK0eGzbMTjrB5aTJRlP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Fine-tune pretrained models<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Fine-tuning BERT for classification<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"qyGsqU_wT0qV"}},{"cell_type":"markdown","source":["# Importing the IMDB dataset"],"metadata":{"id":"R5eX-7r21EdS"}},{"cell_type":"code","source":["# run this code, then restart the python session (and then comment it out)\n","# !pip install -U datasets huggingface_hub fsspec\n","from datasets import load_dataset, DatasetDict\n","dataset = load_dataset('imdb')"],"metadata":{"id":"hso7UFL4Q_zh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vg1-YEIb1HkI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# And back to our regularly scheduled program"],"metadata":{"id":"pkhhOlH21Hhi"}},{"cell_type":"code","source":["# typical python libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# pytorch libraries\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","\n","# huggingface libraries\n","from transformers import BertModel, BertTokenizer"],"metadata":{"id":"-mhNmp9CT17A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import BERT pretrained model\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","bert = BertModel.from_pretrained('bert-base-uncased').to(device)"],"metadata":{"id":"zGSG7ILmT13L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert # note the shape of the final layer"],"metadata":{"id":"svn2dH-6XEti"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert.embeddings.dropout.p"],"metadata":{"id":"IFQaXFzJ0IzP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.vocab_size"],"metadata":{"id":"pNsLMHkRXEqi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qgLrIblCXEnk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# More on model inputs and outputs"],"metadata":{"id":"GkIpy9laXEkf"}},{"cell_type":"code","source":["text = 'Replace me with any text you like.'\n","tokens = tokenizer(text, return_tensors='pt').to(device)\n","tokens"],"metadata":{"id":"qnqzO6RET8Cy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get model output using specific inputs\n","output = bert(\n","    input_ids      = tokens['input_ids'],\n","    attention_mask = tokens['attention_mask']\n","    )\n","\n","\n","# but there's a better way ;)\n","output = bert(**tokens)"],"metadata":{"id":"PcPR5iAwy1yU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dir(output)"],"metadata":{"id":"owgAcbD-zXnu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output['last_hidden_state'].shape"],"metadata":{"id":"e0U9marqT10H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output['pooler_output'].shape"],"metadata":{"id":"pI9zqZ7tT1xa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# can we get text output?\n","bert.generate(tokens, max_length=100, do_sample=True).cpu()"],"metadata":{"id":"0HIwSby8T1u9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jWk8pPKBT1sM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create an LLM model using pretrained BERT with a new head"],"metadata":{"id":"mQ0JGbZ3T1pm"}},{"cell_type":"code","source":["class BertForBinaryClassification(nn.Module):\n","  def __init__(self, num_labels=2):\n","    super(BertForBinaryClassification, self).__init__()\n","\n","    # Load the pre-trained BERT model.\n","    self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","    # classification head that converts the 768-d pooled output into 2 final outputs.\n","    self.classifier = nn.Linear(768,2)\n","    self.dropout = nn.Dropout(self.bert.embeddings.dropout.p) # 10%\n","\n","    # initialize the weights and biases\n","    nn.init.xavier_uniform_(self.classifier.weight)\n","    nn.init.zeros_(self.classifier.bias)\n","\n","\n","  def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n","\n","    # forward pass through the downloaded (pretrained) BERT\n","    outputs = self.bert(\n","      input_ids      = input_ids,\n","      attention_mask = attention_mask,\n","      token_type_ids = token_type_ids)\n","\n","    # extract the pooled output and apply dropout\n","    pooled_output = self.dropout( outputs.pooler_output )\n","\n","    # final push through the classification layer.\n","    logits = self.classifier(pooled_output)\n","    return logits\n"],"metadata":{"id":"05j-M4MRb4Iy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create an instance of the model and test it\n","model = BertForBinaryClassification().to(device)\n","\n","# test the output\n","tokens = tokenizer(text, return_tensors='pt').to(device)\n","out = model(**tokens)\n","out"],"metadata":{"id":"SLb0HwWLciY2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dMKtr8CWeK72"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import the dataset"],"metadata":{"id":"KSEbXYTweK5M"}},{"cell_type":"code","source":["# check out the dataset\n","dataset"],"metadata":{"id":"wLWXY0dnb4AA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset['train'][2000]"],"metadata":{"id":"C-vopllDeVku"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,3.5))\n","plt.plot(dataset['train']['label'] + np.random.randn(len(dataset['train']))/20,'m.',markersize=1,alpha=.2)\n","\n","plt.gca().set(xlabel='Review index',ylabel='Label',yticks=[0,1],yticklabels=['Negative','Positive'],\n","              xlim=[0,len(dataset['train'])],ylim=[-.5,1.5])\n","plt.show()"],"metadata":{"id":"vA77mq5YeVhR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# It's a lot of data; let's take a small sample"],"metadata":{"id":"Nl-Ad6eT4JbL"}},{"cell_type":"code","source":["dataset['train'].select(range(100))"],"metadata":{"id":"kW8Is7Mi4JWo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reduce the dataset size while:\n","# 1) including both categories (see plot above and range() below)\n","# 2) preserving only 'train' and 'test'\n","# 2) using a DatasetDict (not a Python dict) to preserve methods\n","small_data = DatasetDict({split:dataset[split].select(range(10000,15000)) for split in ['train','test']})\n","small_data"],"metadata":{"id":"rduBQspG4OzL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confirm we still have both categories\n","plt.figure(figsize=(10,3.5))\n","plt.plot(small_data['train']['label'] + np.random.randn(len(small_data['train']))/20,'m.',markersize=1)\n","\n","plt.gca().set(xlabel='Review index',ylabel='Label',yticks=[0,1],yticklabels=['Negative','Positive'],\n","              xlim=[0,len(small_data['train'])],ylim=[-.5,1.5])\n","plt.show()"],"metadata":{"id":"vQj1bDeB4pMM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Bg7jmIBo4JUB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokenizing the text with padding"],"metadata":{"id":"6W7QFDMH4JOa"}},{"cell_type":"code","source":["# this works...\n","first_try = tokenizer(dataset['train'][0]['text'])\n","\n","# but this is better b/c reviews have differing lengths\n","better = tokenizer(\n","    dataset['train'][0]['text'], # the text to tokenize\n","    max_length = 512,            #\n","    padding    = 'max_length',   # using pad_token to reach max_len\n","    truncation = True)           # cut out tokens >max_len\n","\n","\n","print(f\"'Naive' tokenization (N={len(first_try['input_ids'])}):\")\n","print(f\"{first_try['input_ids']}\")\n","\n","print(f\"\\nBetter tokenization (N={len(better['input_ids'])}):\")\n","print(f\"{better['input_ids']}\")"],"metadata":{"id":"Zqo8ZYCCf0Hs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define a tokenization function that processes each data sample\n","def tokenize_function(one_sample):\n","  return tokenizer(\n","    one_sample['text'],\n","    max_length = 512,         # Maximum sequence length\n","    padding    = 'max_length',# Pad sequences to the maximum length\n","    truncation = True)        # Truncate sequences longer than max_length\n","\n","\n","# apply the tokenization function to the dataset (batched for efficiency)\n","tokenized_dataset = small_data.map(tokenize_function, batched=True)\n","tokenized_dataset"],"metadata":{"id":"kJh6cuaYePG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# remove text pair\n","tokenized_dataset = tokenized_dataset.remove_columns(['text'])\n","\n","# change format to pytorch tensors\n","tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n","\n","# create DataLoaders for training and testing\n","train_dataloader = DataLoader(tokenized_dataset['train'], shuffle=True, batch_size=32)\n","test_dataloader  = DataLoader(tokenized_dataset['test'], batch_size=32)"],"metadata":{"id":"HDtF1zWi2fJc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check dataset structure again\n","print(tokenized_dataset,'\\n\\n')\n","\n","# tokenized_dataset['train'][1000]"],"metadata":{"id":"vDcdKwb5mUta"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# and check out a dataloader iteration\n","X = next(iter(train_dataloader))\n","X"],"metadata":{"id":"dXukKr5Bb39E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DglXfXWcb36U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Now to fine-tune the model"],"metadata":{"id":"Ftzlsxq_b33g"}},{"cell_type":"code","source":["# optimizer and loss function\n","optimizer = torch.optim.AdamW(model.parameters(),lr=1e-5)\n","loss_fun = nn.CrossEntropyLoss() # (cross-entropy loss for multi-class classification)"],"metadata":{"id":"LrR83zjvoy-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get a batch of data\n","batch = next(iter(train_dataloader))\n","\n","# and move it to the GPU\n","tokenz  = batch['input_ids'].to(device)\n","att_msk = batch['attention_mask'].to(device)\n","labels  = batch['label'].to(device)\n","\n","# clear the previous gradients\n","optimizer.zero_grad()\n","\n","# forward pass and get model predictions\n","logits = model(tokenz, attention_mask=att_msk)\n","predLabels = torch.argmax(logits, dim=1)\n","\n","# calculate and store loss + average accuracy\n","loss = loss_fun(logits, labels)\n","train_accuracy = (predLabels == labels).sum().item()/train_dataloader.batch_size\n","\n","# backward pass\n","loss.backward()\n","\n","# update the weights and the learning rate\n","optimizer.step()"],"metadata":{"id":"v5m3eWnynMeb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'Logits are of size {logits.shape} and are:\\n',logits)"],"metadata":{"id":"B9VzgEzjnMYZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Model predictions:',predLabels)\n","print('True labels:',labels)"],"metadata":{"id":"9G_B71mZ7FNe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracy = (predLabels==labels).sum()/train_dataloader.batch_size\n","\n","print(f'Accuracy is {100*accuracy:.1f}%')"],"metadata":{"id":"Stgdg-FT7FK1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kotmoU_u7FFT"},"execution_count":null,"outputs":[]}]}