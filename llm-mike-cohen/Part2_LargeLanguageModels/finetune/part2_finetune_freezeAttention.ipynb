{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyORIDtFLSZlItU1eaIEj3SS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Fine-tune pretrained models<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Partial fine-tuning by freezing attention weights<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"wzuj7gIJkgWr"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from transformers import AutoModelForCausalLM,GPT2Tokenizer\n","import requests"],"metadata":{"id":"aoocnKDi-2RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load pretrained GPT-2 model and tokenizer\n","gpt2 = AutoModelForCausalLM.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"bHKfbxSx-B5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hyperparameters\n","seq_len    = 256 # max sequence length\n","batch_size =  16\n","\n","# use GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"7AInxn0_kzOj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenize Gulliver's travels\n","text = requests.get('https://www.gutenberg.org/cache/epub/829/pg829.txt').text\n","gtTokens = tokenizer.encode(text,return_tensors='pt')[0]"],"metadata":{"id":"lP_txTZrkzLn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SFVusrX5qM1k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Freeze all attention head weights"],"metadata":{"id":"W36VFhVsVc_Z"}},{"cell_type":"code","source":["for name,param in gpt2.named_parameters():\n","  if '.h.' in name:\n","    param.requires_grad = False\n","    print(f'--- Layer {name} is frozen (.requires_grad = {param.requires_grad}).')\n","\n","  elif '.h.' not in name:\n","    print(f'+++ Layer {name} is trainable (.requires_grad = {param.requires_grad}).')"],"metadata":{"id":"8AxhjmrvVg5U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# as a sanity-check, grab weights from one frozen and one trainable layer\n","frozenW_pre = gpt2.transformer.h[6].mlp.c_fc.weight.data\n","trainW_pre  = gpt2.transformer.ln_f.weight.data"],"metadata":{"id":"JrT4eSWrW8Jk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# move the model to the GPU\n","gpt2 = gpt2.to(device)"],"metadata":{"id":"QlWxZTAaky6q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BLpOjjndmitp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fine-tune the model"],"metadata":{"id":"wkXTHacS2HyC"}},{"cell_type":"code","source":["# create the optimizer functions\n","optimizer = torch.optim.AdamW(gpt2.parameters(), lr=5e-5, weight_decay=.01)\n","\n","# Note: don't need the loss function here, because it's calculated internally in the model (thanks HF :D )"],"metadata":{"id":"qpcC47LUy7w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_samples = 123\n","\n","# initialize losses\n","train_loss = []\n","\n","for sampli in range(num_samples):\n","\n","  # get a batch of data\n","  ix = torch.randint(len(gtTokens)-seq_len,size=(batch_size,))\n","  X  = gtTokens[ix[:,None] + torch.arange(seq_len)].to(device)\n","\n","  # forward pass (Hugging Face shifts X internally to get y)\n","  gpt2.zero_grad()\n","  outputs = gpt2(X,labels=X)\n","  loss = outputs.loss\n","\n","  # backprop\n","  loss.backward()\n","  optimizer.step()\n","\n","  # store the per-sample loss\n","  train_loss.append( loss.item() )\n","\n","  # update progress display\n","  if sampli%27==0:\n","    print(f'Sample {sampli:4}/{num_samples}, train loss: {train_loss[-1]:.4f}')"],"metadata":{"id":"WUYuqpThHZL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the losses\n","plt.figure(figsize=(8,3))\n","plt.plot(train_loss,'k',markersize=8,label='Train loss')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Data sample',ylabel='Loss')\n","plt.show()"],"metadata":{"id":"5XdjwacoHZOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5gok0UnHXQB_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# grab the weight matrices again\n","frozenW_pst = gpt2.transformer.h[6].mlp.c_fc.weight.data.cpu()\n","trainW_pst  = gpt2.transformer.ln_f.weight.data.cpu()"],"metadata":{"id":"OtbSdnl6XP_X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# should be all zeros if the layer was frozen\n","print('Frozen layer, norm(post-pre):')\n","print('  ',torch.norm(frozenW_pst - frozenW_pre))\n","\n","print('\\nTrainable layer, norm(post-pre):')\n","print('  ',torch.norm(trainW_pst - trainW_pre))"],"metadata":{"id":"vkbGVHeVofFw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LW_z-amTXhST"},"execution_count":null,"outputs":[]}]}