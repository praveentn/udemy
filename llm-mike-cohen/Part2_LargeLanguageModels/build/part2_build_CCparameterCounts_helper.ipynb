{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNnaqirtiO8Vcp6xdxWIfn2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 2:</h2>|<h1>Large language models<h1>|\n","|<h2>Section:</h2>|<h1>Build a GPT<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: How many parameters?<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"sNed4ojySRDt"}},{"cell_type":"code","source":[],"metadata":{"id":"m6CYKu08DpZB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mc4CQP5L-HGR"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","\n","!pip install torchinfo\n","from torchinfo import summary\n","\n","# svg plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["# libraries for GPT models and their tokenizer\n","from transformers import AutoModelForCausalLM,GPT2Tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"aoocnKDi-2RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"x8g6CMD3UT44"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Import GPT2 models"],"metadata":{"id":"vLQsU4z9UT2S"}},{"cell_type":"code","source":["# dictionary of modelname:identifier\n","model_ids = {\n","    'small':  'gpt2',        # 124M\n","    'medium': 'gpt2-medium', # 355M\n","    'large':  'gpt2-large',  # 774M\n","    'xl':     'gpt2-xl'      # 1.6B (including this model really slows things down...)\n","}\n","\n","# load all models into a dictionary\n","models = {}\n","for name, id in model_ids.items():\n","  models[name] = AutoModelForCausalLM.from_pretrained(id)"],"metadata":{"id":"BD5QgJ0XUMRJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models.keys()"],"metadata":{"id":"qkhc1RIXRCOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now you can iterate over them\n","for name, model in models.items():\n","  print(f'{sum():13,} parameters in gpt2-{name}')"],"metadata":{"id":"zkbJ5akxXBi-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sl8Rg_sYRhE7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Compare GPT2-small against \"model 5\""],"metadata":{"id":"VZPynyQPRhCW"}},{"cell_type":"code","source":["# same data for all exercises\n","x = tokenizer.encode('This is a test. One that we have done countless times before.',return_tensors='pt')\n","x"],"metadata":{"id":"5Vv9S7wRPyKi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# summary of model and parameters\n","sumry = summary(, input_data=x, col_names=['input_size','output_size','num_params'])\n","print(sumry)"],"metadata":{"id":"tih5Ien8DpWP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["finalHeadCount = # hint: .lm_head.weight\n","\n","print(f'Total trainable parameters: { - :,}')"],"metadata":{"id":"f_Na9G6vRttC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show that token embedding and final output layer are the same\n","plt.figure(figsize=(8,4))\n","\n","tokW = models['small'].\n","head = models['small'].\n","\n","plt.plot(tokW,color=[.9,.7,.9],label='Token embeddings')\n","plt.plot(head,'o',markerfacecolor=[.9,.9,.7],markersize=3,label='Unembeddings')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Dimension',ylabel='Value',xlim=[0,len(tokW)],\n","              title=f'Correlation = {np.corrcoef(tokW,head)[0,1]:.3f}')\n","\n","plt.show()"],"metadata":{"id":"JMXA34B-xXWD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9u-PNB1OyW05"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Weight and bias parameters, as percentage of total\n","\n"],"metadata":{"id":"5q_WcW_YRtqS"}},{"cell_type":"code","source":["for name,mat in models['small'].named_parameters():\n","  print(name)"],"metadata":{"id":"Cs6LfkaPR7wO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize\n","param_counts = np.zeros()\n","\n","for idx,(modelname,model) in enumerate(models.items()):\n","\n","  # get the summary for all parameters\n","  sumry =\n","  finalHeadCount =\n","  param_counts[idx,2] = sumry.total_params - finalHeadCount\n","\n","  # loop through all parameters and increment the parameter count\n","  for layername,mat in model.named_parameters():\n","    if 'weight' in layername:\n","      param_counts\n","    elif 'bias' in layername:\n","      param_counts\n","\n","  print(f'\\n** Model \"{modelname}\":')\n","  print(f'Total weights: {param_counts[idx,0]:13,d} ({  :6.3f}% of all params)')\n","  print(f'Total biases:  {} ({  :6.3f}% of all params)')"],"metadata":{"id":"u12cXVHhDpTi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nfvJ3aqyDpQk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Parameters in attention vs. MLP blocks"],"metadata":{"id":"zRaUdXN2DpN2"}},{"cell_type":"code","source":["# initialize\n","param_counts = np.zeros((len(models.keys()),3),dtype=int)\n","\n","for idx,(modelname,model) in enumerate(models.items()):\n","\n","  # get the summary for all parameters\n","\n","\n","  # loop through all parameters and increment parameter count\n","\n","  print(f'\\n** Model \"{modelname}\" ({len(model.transformer.h)} transformer blocks):')\n","  print(f'Att weights: {param_counts[idx,0]:11,d} ({100*param_counts[idx,0]/param_counts[idx,2]:5.2f}% of all params)')\n","  print(f'MLP weights: {param_counts[idx,1]:11,d} ({100*param_counts[idx,1]/param_counts[idx,2]:5.2f}% of all params)')"],"metadata":{"id":"c54cUFVbDpLP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"emXxnRIDVEe8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.bar(np.arange(0,7,2)-.4,,label='Attention',color=[.85,.4,.02])\n","plt.bar(np.arange(0,7,2)+.4,,label='MLP',color=[.3,.01,.42])\n","\n","plt.legend()\n","plt.gca().set(xticks=range(0,7,2),xticklabels=models.keys(),xlabel='GPT2 model version',\n","              ylabel='Percent of total model weights',title='Percentage weights per layer type')\n","plt.show()"],"metadata":{"id":"-AFqACbJDpId"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E7v5E-0zDpFs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: How many layernorm parameters?"],"metadata":{"id":"8m-WXkt-DpC-"}},{"cell_type":"code","source":[],"metadata":{"id":"nXaTfgf7DpAc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AbPI_4fi7m_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"x9U2EHtx7m8p"},"execution_count":null,"outputs":[]}]}