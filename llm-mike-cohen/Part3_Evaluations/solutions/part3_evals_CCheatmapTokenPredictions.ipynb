{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyP6JFFxC4f/s8dOA1fNSy4t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 3:</h2>|<h1>Evaluating LLMs<h1>|\n","|<h2>Section:</h2>|<h1>Qualitative evaluations<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Visualize single-token predictions<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"urGeUxued5Qf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6kR6FqjfpeL"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","import torch\n","import torch.nn.functional as F\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"3i386vZJ49_Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Tokenize and forward-pass in GPT2-small and -large"],"metadata":{"id":"RpYWC1Uu499X"}},{"cell_type":"code","source":["# load pretrained GPT-2 model and tokenizer\n","gpt2_small = AutoModelForCausalLM.from_pretrained('gpt2')\n","gpt2_large = AutoModelForCausalLM.from_pretrained('gpt2-large')\n","gpt2_small.eval()\n","gpt2_large.eval()\n","\n","# and the tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"PsrX9u8K3AEG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the text\n","text = 'The goal of a correlation analysis is to compute a correlation coefficient. This coefficient is indicated using r, and is a number that encodes the normalized strength of the linear relationship between two variables. The normalization imposes boundaries of -1 to +1. Negative, zero, and positive correlation coefficients have distinct interpretations.'\n","tokens = tokenizer.encode(text,return_tensors='pt')\n"],"metadata":{"id":"CcCcEIMek9X5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get the outputs of the models\n","with torch.no_grad():\n","  outputs_small = gpt2_small(tokens)\n","  outputs_large = gpt2_large(tokens)"],"metadata":{"id":"hyzluVtYefT_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"e106v431innl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Calculate and visualize softmax per-token"],"metadata":{"id":"bvHz8xjr5Dpi"}},{"cell_type":"code","source":["# get the log-softmax logits for each token\n","tokenSM = np.zeros((len(tokens[0]),2))\n","\n","\n","for toki in range(1,len(tokens[0])):\n","\n","  # get the logit outputs and convert to log-softmax\n","  # use the PREVIOUS token position, bc that predicts the current token choice\n","  tokenlogit = outputs_small.logits[0,toki-1,:]\n","  sm = F.log_softmax(tokenlogit,dim=-1)\n","\n","  # extract the softmax for the actual token\n","  tokenSM[toki,0] = sm[tokens[0,toki]].item()\n","\n","\n","\n","  ### repeat for the large model\n","  tokenlogit = outputs_large.logits[0,toki-1,:]\n","  sm = F.log_softmax(tokenlogit,dim=-1)\n","  tokenSM[toki,1] = sm[tokens[0,toki]].item()\n"],"metadata":{"id":"lWvMdupCefZY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,5))\n","\n","# plot the softmax probs\n","axs[0].plot(np.exp(tokenSM[:,0]),'o',label='GPT2 small')\n","axs[0].plot(np.exp(tokenSM[:,1]),'s',label='GPT2 large')\n","\n","# lines connecting the models' predictions\n","for i in range(len(tokenSM)):\n","  axs[0].plot([i,i],[np.exp(tokenSM[i,0]),np.exp(tokenSM[i,1])],color=[.7,.7,.7],zorder=-10)\n","\n","\n","axs[0].legend()\n","axs[0].set(xlabel='Token index',ylabel='Softmax prob',title='Softmax prediction strengths')\n","\n","# compare the two models\n","R = np.corrcoef(tokenSM.T)\n","axs[1].plot(tokenSM[1:,0],tokenSM[1:,1],'ko',markerfacecolor=[.7,.7,.7],alpha=.5,markersize=10)\n","axs[1].plot([-10,0],[-10,0],'k--',linewidth=.5)\n","axs[1].set(xlabel='GPT2 small (log-sm)',ylabel='GPT2 large (log-sm)',title=f'Consistency of predictions (r = {R[0,1]:.2f})')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"hzPQAhUb3KIC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PwN1KrDgeffQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Visualize token predictions as heatmap"],"metadata":{"id":"Ct7gRtvo5Kcb"}},{"cell_type":"code","source":["# get width of one letter\n","fig,ax = plt.subplots(figsize=(10,2))\n","\n","# draw a text object\n","temp_text = ax.text(0,0,'n',fontsize=12,fontfamily='monospace')\n","\n","# Get its bounding box in display coordinates\n","bbox = temp_text.get_window_extent(renderer=fig.canvas.get_renderer())\n","\n","# convert from display to axis coordinates\n","inv = ax.transAxes.inverted()\n","bbox_axes = inv.transform([[bbox.x0,bbox.y0], [bbox.x1,bbox.y1]])\n","en_width = bbox_axes[1,0] - bbox_axes[0,0] # bbox is [(x0,y0),(x1,y1)]\n","\n","plt.close(fig)"],"metadata":{"id":"xqNq-12RefiD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wRQ4NVNsvg7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# min/max scale the logsm\n","logsmScale = np.zeros_like(tokenSM)\n","\n","for i in range(2):\n","  y = tokenSM[1:,i] # ignore the first value b/c 0 -> 1\n","  logsmScale[1:,i] = (y-y.min()) / (y.max()-y.min())"],"metadata":{"id":"O22rUGGwfqEq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokCount = 0\n","\n","x_pos = 0  # starting x position (in axis coordinates)\n","y_pos = 1  # vertical center\n","\n","\n","# setup the figure\n","fig, axs = plt.subplots(2,1,figsize=(10,6))\n","axs[0].axis('off')\n","axs[1].axis('off')\n","\n","for toki in range(len(tokens[0])):\n","\n","  # text of this token\n","  toktext = tokenizer.decode([tokens[0,toki]])\n","\n","  # width of the token\n","  token_width = en_width*len(toktext)\n","\n","  # text object with background color matching the \"activation\"\n","  axs[0].text(x_pos+token_width/2, y_pos, toktext, fontsize=12, ha='center', va='center',fontfamily='monospace',\n","          bbox = dict(boxstyle='round,pad=.3', facecolor=mpl.cm.Reds(logsmScale[toki,0]), edgecolor='none', alpha=.8))\n","\n","  axs[1].text(x_pos+token_width/2, y_pos, toktext, fontsize=12, ha='center', va='center',fontfamily='monospace',\n","          bbox = dict(boxstyle='round,pad=.3', facecolor=mpl.cm.Blues(logsmScale[toki,1]), edgecolor='none', alpha=.8))\n","\n","\n","  # update the token counter and x_pos\n","  tokCount += 1\n","  x_pos += token_width + .015 # plus a small gap\n","\n","  # end of the line; reset coordinates and counter\n","  if tokCount>=20:\n","    y_pos -= .2\n","    x_pos = 0\n","    tokCount = 0\n","\n","plt.show()"],"metadata":{"id":"-gAu4xCkefko"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XHMKcbTTef33"},"execution_count":null,"outputs":[]}]}