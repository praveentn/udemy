{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOdKGEQmX8yhTryziV4dPWx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 3:</h2>|<h1>Evaluating LLMs<h1>|\n","|<h2>Section:</h2>|<h1>Quantitative evaluations<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Masked word prediction in BERT<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"Wwuy5roHLzxN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn.functional as F\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForMaskedLM\n","\n","# Load pre-trained BERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n","model.eval()"],"metadata":{"id":"vEwYHoqWz0nB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WivjQFTfVZo7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# An unmasked sentence in BERT"],"metadata":{"id":"3CSzp20AVZmK"}},{"cell_type":"code","source":["# text is paraphrased from https://en.wikipedia.org/wiki/Cubism\n","text = 'Cubism is an art movement that sparked innovations in music and architecture'\n","tokens = tokenizer.encode(text, return_tensors='pt')\n","\n","for t in tokens[0]:\n","  print(f'Token {t:5} is \"{tokenizer.decode(t)}\"')"],"metadata":{"id":"8y8iC98iCgG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# forward pass\n","with torch.no_grad():\n","  outputs = model(tokens)\n","\n","# find max-logit prediction for token 5\n","logits5 = outputs.logits[0,5,:]\n","maxlogit5 = torch.argmax(logits5)\n","\n","# visualize (logit of token, not preceeding!)\n","plt.figure(figsize=(10,4))\n","plt.plot(maxlogit5,logits5[maxlogit5],'go',markersize=10)\n","plt.plot(logits5,'k.',alpha=.3)\n","\n","plt.gca().set(title=f'Model prediction is \"{tokenizer.decode(maxlogit5)}\" (text is \"{tokenizer.decode(tokens[0,5])}\")',\n","              xlabel='Token index',ylabel='Model output logit',xlim=[-10,tokenizer.vocab_size+9])\n","\n","plt.show()"],"metadata":{"id":"VFn17-DXOdVi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8oiq2B4pOdSk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Using a mask"],"metadata":{"id":"IATkq99FQK5l"}},{"cell_type":"code","source":["masked_text = 'Cubism is an [MASK] movement that sparked innovations in music and architecture'\n","masked_tokens = tokenizer.encode(masked_text, return_tensors='pt')\n","\n","for t in masked_tokens[0]:\n","  print(f'Token {t:5} is \"{tokenizer.decode(t)}\"')\n","\n","# index of the [MASK] token\n","mask_token_idx = torch.where(masked_tokens == tokenizer.mask_token_id)[1]\n","print(f'\\nMask token is {tokenizer.mask_token_id} and is in index {mask_token_idx.item()}')"],"metadata":{"id":"kJiUViJbOZl5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# forward pass\n","with torch.no_grad():\n","  outputs = model(masked_tokens)\n","\n","# find max-logit prediction for masked token position\n","logitsMask = outputs.logits[0,mask_token_idx,:].squeeze()\n","maxlogitMask = torch.argmax(logitsMask)\n","\n","# visualize (logit of token, not preceeding!)\n","plt.figure(figsize=(10,4))\n","plt.plot(maxlogitMask,logitsMask[maxlogitMask],'go',markersize=10)\n","plt.plot(logitsMask,'k.',alpha=.3)\n","\n","plt.gca().set(title=f'Model prediction is \"{tokenizer.decode(maxlogitMask)}\" (text is \"{tokenizer.decode(tokens[0,mask_token_idx])}\")',\n","              xlabel='Token index',ylabel='Model output logit',xlim=[-10,tokenizer.vocab_size+9])\n","\n","plt.show()"],"metadata":{"id":"gTByhO9rQa3U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print the top-10 predictions\n","\n","# friendly reminder of the masked text\n","print(masked_text,'\\n')\n","\n","# get the top 10\n","vals,toks = torch.topk(logitsMask,10)\n","\n","# print\n","for t,val in zip(toks,vals):\n","  print(f'Logit score of {val:.2f} for token \"{tokenizer.decode(t)}\"')"],"metadata":{"id":"xJSYUTJVQa0h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yiVs-7KcQavZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loop over all tokens to get predictions"],"metadata":{"id":"SIDYU3KrQaqP"}},{"cell_type":"code","source":["# initialize\n","predicted_tokens = np.zeros(len(tokens[0]),dtype=int)\n","\n","# loop over tokens, replace with [MASK], and get logits\n","for idx,tok in enumerate(tokens[0]):\n","\n","  # make a copy and replace a token with mask\n","  masked_tokens = tokens.clone()\n","  masked_tokens[0,idx] = tokenizer.mask_token_id\n","\n","  # confirmation:\n","  print([t.item() for t in masked_tokens[0]])\n","\n","  # forward pass through the model\n","  with torch.no_grad(): outputs = model(masked_tokens)\n","\n","  # get logits for the masked position\n","  mask_logits = outputs.logits[0,idx,:].squeeze()\n","\n","  # get the max masked prediction and its z-score\n","  predicted_tokens[idx] = torch.argmax(mask_logits,dim=-1)"],"metadata":{"id":"UxLlSONpNmDd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"G63T8-QDu9Fd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Check the final results!"],"metadata":{"id":"4tbdgSagwBJW"}},{"cell_type":"code","source":["print('Original text:\\n',text,'\\n')\n","print('Predicted text:\\n',' '.join([tokenizer.decode(t) for t in predicted_tokens[1:-1]]))"],"metadata":{"id":"lvCxiLQHNl_-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('     ORIGINAL |  PREDICTED | EMBEDS CS')\n","print('-'*39)\n","for tidx in range(1,len(tokens[0])-2):\n","\n","  target = tokens[0,tidx]\n","  prediction = predicted_tokens[tidx]\n","\n","  # cosine similarity between the predicted and original token\n","  tokenE = model.bert.embeddings.word_embeddings.weight[target,:].detach()\n","  predE = model.bert.embeddings.word_embeddings.weight[prediction,:].detach()\n","\n","  cs = torch.cosine_similarity(tokenE.unsqueeze(0),predE.unsqueeze(0))\n","\n","  print(f' {tokenizer.decode(target):>12} | {tokenizer.decode(prediction):^10} |  {cs.item():.3f}')"],"metadata":{"id":"jbdm5XSxNl38"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ChygtJhctyVm"},"execution_count":null,"outputs":[]}]}