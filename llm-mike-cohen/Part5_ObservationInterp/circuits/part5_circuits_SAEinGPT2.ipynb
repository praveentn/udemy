{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMRs1z7Kdha+fS6nmkSX9Nd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><b><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></b></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Identifying circuits and components<h1>|\n","|<h2>Lecture:</h2>|<h1><b>SAE in GPT2 learns about Hungarian Palinka<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"mv5YXPmTJfJl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4kIpBsXzV4US"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","import matplotlib.gridspec as gridspec\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# model & tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model     = AutoModelForCausalLM.from_pretrained('gpt2')\n","model.eval().to(device)"],"metadata":{"id":"LMvw37uMqJQc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AR9tReRzlpV7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hook the MLP activations\n","activations = {}\n","\n","def mlp_hook(module, input, output):\n","  activations[f'mlp_{layer2hook}'] = output.detach()\n","\n","# hook a middle layer\n","layer2hook = model.config.n_layer//2\n","handle = model.transformer.h[layer2hook].mlp.c_fc.register_forward_hook(mlp_hook)\n","\n","# note: I'm setting a handle to the hook to remove it later"],"metadata":{"id":"VUolDrVBV66Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"K73naDTYnYH8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokens and activations"],"metadata":{"id":"m0bTR3J9nYFQ"}},{"cell_type":"code","source":["# https://en.wikipedia.org/wiki/P%C3%A1linka\n","texts =  'Palinka is a traditional fruit spirit (or fruit brandy) with origins in the medieval Hungary, known under several names. Protected as a geographical indication of the European Union, only fruit spirits mashed, distilled, matured and bottled in Hungary and similar apricot spirits from four provinces of Austria can be called \"palinka\". A similar product exists in the Czech Republic and Slovakia where it is known as palenka, and in Romania (Transylvania), Italy, and Greece under the name palinca. While palinka is traditionally made from a mash of ripe fruit, the law does not control the addition of non-concentrated fruit juice, and explicitly allows the use of fruit pulp. Dried fruits are excluded from the mash only, and may be used in the aging process. While palinka has PDO on its own, some regions of Hungary are especially suitable for the production of certain fruits, and palinka of excellent quality has been produced in those regions for centuries. These local variations are protected as separate geographical indications and have their own well-detailed regulations.'\n","\n","tokens = tokenizer.encode(texts, return_tensors='pt').to(device)\n","numtoks = len(tokens[0])\n","numtoks"],"metadata":{"id":"IxdZnQhvWAta"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run the model to get the activations\n","with torch.no_grad():\n","  model(tokens)\n","\n","# remove the hook\n","handle.remove()"],"metadata":{"id":"sqVP0eFyumBy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# grab the activations\n","X = activations[f'mlp_{layer2hook}'].squeeze()\n","print(f'Activations size: {X.shape}')\n","\n","# move to GPU\n","X = X.to(device)"],"metadata":{"id":"miVC8vVDjvW_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yCOpK8xUWC9B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create and train the autoencoder"],"metadata":{"id":"T_TPzU3_naiU"}},{"cell_type":"code","source":["class SparseAE(nn.Module):\n","  def __init__(self, input_dim, latent_dim, k=None, sparsity_weight=1, decor_weight=.0005):\n","    super().__init__()\n","    self.encoder = nn.Linear(input_dim, latent_dim, bias=False)\n","    # self.decoder = nn.Linear(latent_dim, input_dim, bias=False)\n","    # note: I've tied the weights by transposing the encoder weights in forward()\n","\n","    self.sparsity_weight = sparsity_weight\n","    self.decor_weight = decor_weight\n","\n","    # k-sparse parameter defaults to 50% of input\n","    if k==None:\n","      self.k = input_dim//2\n","    else:\n","      self.k = k\n","\n","  def forward(self, x):\n","\n","    # forward pass to the latent layer\n","    latent = F.relu(self.encoder(x))\n","\n","    # \"k-sparsify\": force sparsity by zeroing out small activations\n","    topk_vals = torch.topk(latent,self.k,dim=1)[0]\n","    thresh = topk_vals[:,-1].unsqueeze(1) # kth-largest value is the smallest of the sorted top-k\n","    mask = (latent >= thresh).float() # mask is 0's and 1's\n","    latent_sparse = latent * mask\n","\n","    # finally, decode via tied weights\n","    y = F.linear(latent_sparse, self.encoder.weight.t())\n","\n","    return y,latent_sparse\n","\n","  def sparsity_loss(self, z):\n","    return self.sparsity_weight * torch.mean(torch.abs(z))\n","\n","  # penalty on inter-latent covariance\n","  def decorrelation_loss(self, estLatent):\n","    cov = torch.cov(estLatent.T)\n","    off_diag = cov - torch.diag(torch.diag(cov))\n","    return self.decor_weight * torch.sum(off_diag**2)\n"],"metadata":{"id":"LQWvtqVWWE4z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create an instance of the autoencoder (kx latent dimensions)\n","num_latent = 2*X.shape[1]\n","ae = SparseAE(input_dim=X.shape[1], k=X.shape[1]//3, latent_dim=num_latent)\n","ae = ae.to(device)"],"metadata":{"id":"n4MekXvbnezO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vEYFqrH3_0mv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training parameters and initializations\n","optimizer = optim.Adam(ae.parameters(), lr=.0001)\n","mse_loss  = nn.MSELoss()\n","\n","n_epochs  = 100\n","losses = np.zeros((n_epochs,3))\n","\n","\n","# train!!\n","for epoch in range(n_epochs):\n","\n","  # forward pass\n","  optimizer.zero_grad()\n","  x_pred,latent = ae(X)\n","\n","  # get the three losses\n","  L1 = ae.sparsity_loss(latent)\n","  deCor = ae.decorrelation_loss(latent)\n","  loss = mse_loss(x_pred,X) + L1 + deCor\n","\n","  # and store them for later inspection\n","  losses[epoch,0] = loss.item()\n","  losses[epoch,1] = L1.item()\n","  losses[epoch,2] = deCor.item()\n","\n","  # do backprop\n","  loss.backward()\n","  optimizer.step()\n","\n","  if epoch%11 == 0:\n","    print(f'Epoch {epoch+1:3d}, loss = {losses[epoch,0]:.4f}')\n","\n","\n","\n","## final run to get latent activations\n","with torch.no_grad():\n","  aeout,latent = ae(X)\n","\n","latent = latent.cpu().numpy()\n","latent.shape"],"metadata":{"id":"MekaXD9xWO6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,3,figsize=(12,3))\n","\n","# the plots\n","axs[0].plot(losses[:,0],'ks-',markerfacecolor=[.9,.7,.7,.5],linewidth=.5)\n","axs[1].plot(losses[:,1],'ko-',markerfacecolor=[.7,.9,.7,.5],linewidth=.5)\n","axs[2].plot(losses[:,2],'k^-',markerfacecolor=[.7,.7,.9,.5],linewidth=.5)\n","\n","# the labeling\n","losslabels = [ 'Total loss','Sparsity loss','Decorrelation loss']\n","for i in range(3):\n","  axs[i].set(xlabel='Epoch',ylabel='Loss',title=losslabels[i])\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"WkRDIQSyqfI2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,3))\n","plt.imshow(latent,vmin=0,vmax=.1,aspect='auto')\n","\n","plt.gca().set(ylabel='Token index',xlabel='Latent component index',title='Latent activations')\n","plt.colorbar(pad=.01)\n","plt.show()"],"metadata":{"id":"aV7_UCeRWRKy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JTaRZQzUvOnK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Quantitative scores for each latent component"],"metadata":{"id":"rrmOXMT0WRDu"}},{"cell_type":"code","source":["# create a mask that is nan for zero-valued activations\n","densitymask = np.full(latent.shape,np.nan)\n","densitymask[latent!=0] = 1\n","\n","# percent of nonzero activations per latent component (plotted in next code cell)\n","latentDensity = 100 * np.nansum(densitymask,axis=0) / densitymask.shape[0]\n","\n","# token-averaged activation, excluding zeros\n","nonzeroAct = np.nanmean(np.abs(latent*densitymask),axis=0)\n","nonzeroAct[np.isnan(nonzeroAct)] = 0\n","\n","# average nonzero activations per latent component\n","densityActivation = latentDensity*nonzeroAct\n","\n","nonzeroAct.shape"],"metadata":{"id":"JpHB-cyDj8pe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,3,figsize=(12,3.3))\n","\n","whichzero = latentDensity==0\n","axs[0].plot(np.where(whichzero)[0],latentDensity[whichzero],'rx',alpha=.1)\n","axs[0].plot(np.where(~whichzero)[0],latentDensity[~whichzero],'ko',markerfacecolor=[.9,.7,.7],alpha=.5)\n","axs[0].set(xlabel='Latent component index',ylabel='% tokens with activation',\n","              title=f'Latent component layer is {100*(1-whichzero.mean()):.2f}% dense')\n","\n","axs[1].plot(np.where(whichzero)[0],nonzeroAct[whichzero],'rx',alpha=.1)\n","axs[1].plot(np.where(~whichzero)[0],nonzeroAct[~whichzero],'ko',markerfacecolor=[.7,.9,.7],alpha=.5)\n","axs[1].set(xlabel='Latent component index',ylabel='Nonzero activations',\n","              title=f'Latent component activations')\n","\n","\n","axs[2].plot(nonzeroAct[~whichzero],latentDensity[~whichzero],'ko',markerfacecolor=[.7,.7,.9],alpha=.5)\n","axs[2].set(xlabel='Nonzero activations',ylabel='% tokens with activation',\n","              title=f'Activations by density')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"yo25Z8fuMPiO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hgdsLfSxR-JZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Qualitative inspection (text heatmaps)"],"metadata":{"id":"7sSLP0jYj81A"}},{"cell_type":"code","source":["# get width of one letter\n","fig,ax = plt.subplots(figsize=(10,2))\n","temp_text = ax.text(0,0,'n',fontsize=12,fontfamily='monospace')\n","bbox = temp_text.get_window_extent(renderer=fig.canvas.get_renderer())\n","inv = ax.transAxes.inverted()\n","bbox_axes = inv.transform([[bbox.x0,bbox.y0], [bbox.x1,bbox.y1]])\n","en_width = bbox_axes[1,0] - bbox_axes[0,0]\n","plt.close(fig)"],"metadata":{"id":"T64KbTO1j8yK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to draw the heatmap, given a latent component\n","def drawTextHeatmap(comp):\n","\n","  # min-max scale\n","  compAct = (latent[:,comp] - latent[:,comp].min()) / (latent[:,comp].max() - latent[:,comp].min())\n","\n","  tokCount = 0\n","\n","  x_pos = 0  # starting x position (in axis coordinates)\n","  y_pos = 1  # vertical center\n","\n","  fig, ax = plt.subplots(figsize=(10,2))\n","  ax.axis('off')\n","\n","  for toki in range(numtoks):\n","\n","    # text of this token\n","    toktext = tokenizer.decode([tokens[0,toki]])\n","\n","    # width of the token\n","    token_width = en_width*len(toktext)\n","\n","    # text object with background color matching the \"activation\"\n","    ax.text(x_pos+token_width/2, y_pos, toktext, fontsize=12, ha='center', va='center',fontfamily='monospace',\n","            bbox = dict(boxstyle='round,pad=.3', facecolor=mpl.cm.Reds(compAct[toki]**2), edgecolor='none', alpha=.8))\n","\n","    # update the token counter and x_pos\n","    tokCount += 1\n","    x_pos += token_width + .01 # plus a small gap\n","\n","    # end of the line; reset coordinates and counter\n","    if tokCount>=20:\n","      y_pos -= .17\n","      x_pos = 0\n","      tokCount = 0\n","\n","  print(f'Latent component {comp}')\n","  plt.show()"],"metadata":{"id":"EtyGIz-oj8ve"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rFSQgZpSvm8T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sort\n","sortByActivity = np.argsort(nonzeroAct)[::-1]\n","sortByTokenDensity = np.argsort(latentDensity)[::-1]\n","\n","latent.shape"],"metadata":{"id":"Fj6qwOPhWRAl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# comment one, run the other\n","drawTextHeatmap(sortByActivity[30])\n","# drawTextHeatmap(sortByTokenDensity[14])"],"metadata":{"id":"iB1wE7BJj8sj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EoNXrywtw54x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Pick a component based on latent activation to a word category"],"metadata":{"id":"Ascea3fYQqeE"}},{"cell_type":"code","source":["# single-token words to look for\n","geography_words = [ 'Hungary','geographical','European','Union',\n","                    'provinces','Austria','Czech','Republic',\n","                    'Slovakia','Romania','Italy','Greece','regions' ]\n","\n","# vector of targets\n","target_tokens = np.zeros(numtoks,dtype=bool)\n","\n","# mark target tokens\n","for w in geography_words:\n","  tokidx = tokenizer.encode(w,add_prefix_space=True)[0]\n","  target_tokens[np.where(tokens[0].cpu() == tokidx)[0]] = 1\n","\n","target_tokens"],"metadata":{"id":"2xWyBWN4KmMO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate a \"geoscore\" ratio of target tokens to non-target tokens\n","geoscore = np.zeros(latent.shape[1])\n","\n","# loop over latent components\n","for comp in range(latent.shape[1]):\n","\n","  # get the target and nontarget activations\n","  targ = latent[target_tokens,comp]\n","  nontarg = latent[~target_tokens,comp]\n","\n","  # geoscore is the ratio of their non-zero token activations\n","  if (nontarg.sum()>0) & (targ.sum()>0):\n","    geoscore[comp] = targ[np.nonzero(targ)].mean() / nontarg[np.nonzero(nontarg)].mean()\n","\n","\n","# visualize the scores\n","plt.figure(figsize=(10,4))\n","plt.plot(geoscore,'ko',markerfacecolor=[.9,.7,.7,.5])\n","plt.gca().set(xlabel='Latent components',ylabel='Score',title='Geography token activation score')\n","plt.show()"],"metadata":{"id":"ubqn_NnQKmIT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# then make a text heatmap!\n","sortByGeoscore = np.argsort(geoscore)[::-1]\n","drawTextHeatmap(sortByGeoscore[9])"],"metadata":{"id":"BxDMEyagKl_k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HDPpswdVKl7y"},"execution_count":null,"outputs":[]}]}