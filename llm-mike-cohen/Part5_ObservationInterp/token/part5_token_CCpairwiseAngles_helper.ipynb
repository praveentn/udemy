{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOupzGwAKrB9WEBe+FLuM/B"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating token embeddings<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Laminar evolution of sequential angular adjustments<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"zWgL25TLsbN5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","import torch\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"jg2ZyWpQJWxR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Model, text, activations"],"metadata":{"id":"mPLtIHgYJWt7"}},{"cell_type":"code","source":["# load GPT2 model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained('gpt2-xl')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n","\n","model.eval()"],"metadata":{"id":"vEwYHoqWz0nB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MTvKXQXK1aJf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sentences generated by Claude.ai\n","sentences = [\n","    \"I saw her at the market.\",\n","    \"She gave her the book.\",\n","    \"They asked her for advice.\",\n","    \"We invited her to dinner.\",\n","    \"The dog followed her home.\",\n","    \"They asked her to join.\",\n","    \"He saw her at the park yesterday.\",\n","    \"Did you give her your address?\",\n","    \"I haven't seen her in ages.\",\n","    \"I told her the truth.\",\n","    \"They congratulated her on his success.\",\n","    \"She recognized her immediately.\",\n","    \"The teacher praised her for his work.\",\n","    \"I met her last summer.\",\n","    \"The child hugged her tightly.\",\n","    \"They warned her about the danger.\",\n","    \"She drove her to the airport.\",\n","    \"We waited for her for hours.\",\n","    \"The cat scratched her accidentally.\",\n","    \"They surprised her with a gift.\",\n","    \"She called her on the phone.\",\n","    \"The jury found her not guilty.\",\n","    \"I remembered her from school.\",\n","    \"They elected her as president.\",\n","    \"She forgave her for his mistake.\",\n","    \"The police questioned her yesterday.\",\n","    \"I helped her with his homework.\",\n","    \"They spotted her in the crowd.\",\n","    \"She visited her in the hospital.\",\n","    \"The manager promoted her last week.\",\n","    \"I trusted her completely.\",\n","    \"They respected her for his honesty.\",\n","    \"She taught her how to swim.\",\n","    \"The bird attacked her suddenly.\",\n","    \"I greeted her warmly.\",\n","    \"They supported her through difficult times.\",\n","    \"She ignored her at the party.\",\n","    \"The judge sentenced her to community service.\",\n","    \"I photographed her during the event.\",\n","    \"They believed her despite the evidence.\",\n","    \"She surprised her on his birthday.\",\n","    \"The guard stopped her at the entrance.\",\n","    \"I missed her terribly.\",\n","    \"They watched her leave the building.\",\n","    \"She accompanied her to the concert.\",\n","    \"The crowd cheered her enthusiastically.\",\n","    \"I described her to the police.\",\n","    \"They thanked her for his help.\",\n","    \"She admired her for his courage.\",\n","    \"The committee nominated her for the award.\",\n","    \"I married her last spring.\",\n","    \"They informed her about the changes.\",\n","    \"She introduced her to the parents.\",\n","    \"The author based the character on her.\"\n","]\n","\n","target_token = tokenizer.encode(' her')[0]\n","print(f'There are {len(sentences)} sentences.')"],"metadata":{"id":"q8Fuw-ug1aBU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# need to specify a padding token\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# tokenize\n","tokens = tokenizer(')\n","\n","# push through the model (~1 min for gpt2-xl in cpu)\n"],"metadata":{"id":"iYx21qIa34uN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_hiddens = len(outputs.hidden_states)\n","num_hiddens"],"metadata":{"id":"gFB2wI5N7d6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JUpXNloJ6F1T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Sequential adjustment angles"],"metadata":{"id":"i1R9c0TSskDR"}},{"cell_type":"code","source":["# get angle of token vector from previous to current token, in each layer\n","\n","angles = np.zeros((num_hiddens,len(sentences),2))\n","\n","for senti in\n","\n","  # find the index of the target token (convert to list, then .index to find)\n","  thisSentenceTokens = # only where there are valid tokens\n","  targidx =\n","\n","  nontargidx = # pick two consecutive random tokens that aren't the target\n","\n","\n","  for layeri in range(num_hiddens):\n","\n","    # TARGET: calculate the angle between this and the previous token activations\n","    v = outputs.hidden_states[layeri][senti,\n","    u = outputs.hidden_states[layeri][senti,\n","    angles[layeri,senti,0] = torch.acos( torch.dot / (torch.linalg.norm * torch.linalg.norm ) )\n","\n","    # NON-TARGET: same as above but for random pairs\n","    v =\n","    u =\n","    angles[layeri,senti,1] =\n","\n","angles.shape"],"metadata":{"id":"AMlv6__4i8R8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(11,4))\n","\n","xticks = np.arange(num_hiddens)\n","\n","# plot all the individual sentences\n","plt.plot(,,color=[.9,.7,.7],alpha=.5)\n","plt.plot(,,color=[.7,.7,.9],alpha=.5)\n","\n","# and the average over sentences\n","plt.plot(,,'r',linewidth=3,label='Target')\n","plt.plot(,,'b',linewidth=3,label='Non-target')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Transformer block',ylabel='Angle (rad.)',xlim=[-.1,num_hiddens],\n","              title=r'$\\Delta v^{\\circ}$ relative to previous token')\n","\n","plt.show()"],"metadata":{"id":"jeu5kAgai8O9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lcaR6qJ2i8I5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Revealing the mystery of the bimodal angles"],"metadata":{"id":"lVm097j5na7L"}},{"cell_type":"code","source":["angles = np.zeros((num_hiddens,len(sentences),4))\n","\n","for senti in range(len(sentences)):\n","  for layeri in range(num_hiddens):\n","    for toki in range(4):\n","\n","      # extract the two vectors\n","      v = # taget token index\n","      u = # the other one\n","\n","      # calculate angle between them\n","      rad = torch.acos(  / () )\n","\n","      # convert to degrees\n","      angles[layeri,senti,toki] =\n","\n","angles.shape"],"metadata":{"id":"XcGx31Scnbac"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(11,4))\n","\n","xticks = np.arange(num_hiddens)\n","\n","for toki in range(angles.shape[-1]):\n","\n","  # plot all the individual sentences\n","  plt.plot(,,color='krbg'[toki],linewidth=.5,alpha=.1)\n","\n","  # and the average over sentences\n","  plt.plot(meansomething,'krbg'[toki],linewidth=3,label=fr'$\\Delta$({toki},{toki+1})')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Transformer layer',ylabel='Angle (deg.)',xlim=[-.1,num_hiddens-1],\n","              title=r'$\\Delta v^{\\circ}$ relative to next token')\n","\n","plt.show()"],"metadata":{"id":"e9robK8Hnbad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rQTcDuLB7HIn"},"execution_count":null,"outputs":[]}]}