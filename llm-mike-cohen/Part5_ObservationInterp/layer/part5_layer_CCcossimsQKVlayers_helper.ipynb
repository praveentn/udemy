{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNgjVNCgUmlHJ01BZWF/0Ie"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating layers<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Token-related similarities across layers<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"RDgvq3mX1Vsh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","import torch\n","import torch.nn.functional as F\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","\n","# vector matplotlib\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"MTvKXQXK1aJf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Load model, implant hooks, get activations"],"metadata":{"id":"4P8rG6tAA4ud"}},{"cell_type":"code","source":["# load GPT2 model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained('gpt2-xl')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","model.eval()\n","\n","# variable for the number of embedding dimensions\n","nEmb ="],"metadata":{"id":"vEwYHoqWz0nB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a hook function to store QVK vectors\n","activations = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","    activations[f'attn_{layer_number}_qvk'] = output.detach().numpy()\n","  return hook\n","\n","\n","# surgery ;)"],"metadata":{"id":"Q3_iseD-1aG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VUoq64u-1aEE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generated by Claude.ai\n","sentences = [\n","    \"I saw her at the market.\",\n","    \"She gave her the book.\",\n","    \"They asked her for advice.\",\n","    \"We invited her to dinner.\",\n","    \"The dog followed her home.\",\n","    \"They asked her to join.\",\n","    \"He saw her at the park yesterday.\",\n","    \"Did you give her your address?\",\n","    \"I haven't seen her in ages.\",\n","    \"I told her the truth.\",\n","    \"They congratulated her on his success.\",\n","    \"She recognized her immediately.\",\n","    \"The teacher praised her for his work.\",\n","    \"I met her last summer.\",\n","    \"The child hugged her tightly.\",\n","    \"They warned her about the danger.\",\n","    \"She drove her to the airport.\",\n","    \"We waited for her for hours.\",\n","    \"The cat scratched her accidentally.\",\n","    \"They surprised her with a gift.\",\n","    \"She called her on the phone.\",\n","    \"The jury found her not guilty.\",\n","    \"I remembered her from school.\",\n","    \"They elected her as president.\",\n","    \"She forgave her for his mistake.\",\n","    \"The police questioned her yesterday.\",\n","    \"I helped her with his homework.\",\n","    \"They spotted her in the crowd.\",\n","    \"She visited her in the hospital.\",\n","    \"The manager promoted her last week.\",\n","    \"I trusted her completely.\",\n","    \"They respected her for his honesty.\",\n","    \"She taught her how to swim.\",\n","    \"The bird attacked her suddenly.\",\n","    \"I greeted her warmly.\",\n","    \"They supported her through difficult times.\",\n","    \"She ignored her at the party.\",\n","    \"The judge sentenced her to community service.\",\n","    \"I photographed her during the event.\",\n","    \"They believed her despite the evidence.\",\n","    \"She surprised her on his birthday.\",\n","    \"The guard stopped her at the entrance.\",\n","    \"I missed her terribly.\",\n","    \"They watched her leave the building.\",\n","    \"She accompanied her to the concert.\",\n","    \"The crowd cheered her enthusiastically.\",\n","    \"I described her to the police.\",\n","    \"They thanked her for his help.\",\n","    \"She admired her for his courage.\",\n","    \"The committee nominated her for the award.\",\n","    \"I married her last spring.\",\n","    \"They informed her about the changes.\",\n","    \"She introduced her to the parents.\",\n","    \"The author based the character on her.\"\n","]\n","\n","target_token = tokenizer.encode(' her')[0]\n","print(f'There are {len(sentences)} sentences.')"],"metadata":{"id":"q8Fuw-ug1aBU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# need to specify a padding token\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# tokenize\n","tokens = tokenizer(sentences,padding=True,return_tensors='pt')\n","\n","# push through the model (~1 min for gpt2-xl in cpu)\n","with torch.no_grad(): model(**tokens)"],"metadata":{"id":"iYx21qIa34uN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# unique values for each layer type\n","qLoc = 1\n","kLoc = 2\n","vLoc = 3\n","\n","# a vector mask\n","vectorMask =\n","\n","# outer product to create a matrix with unique values for each interaction\n","matrixMask = vectorMask[:,None] @ vectorMask[None,:]\n","matrixMask = np.triu(matrixMask,"],"metadata":{"id":"xVnYAo_79JRO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9Oy0nfHoB5qe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: A function to get layer-specific activations"],"metadata":{"id":"IK0K5THlKUJi"}},{"cell_type":"code","source":["## function to get activations for target and non-target tokens\n","\n","def get_QKV_activations(whichlayer):\n","\n","  # initialize\n","  actsAll_trg = np.zeros(\n","  actsAll_non =\n","\n","  # loop over sentences b/c target position varies\n","  for senti in range(len(sentences)):\n","\n","    # find the index of the target token (convert to list, then .index to find)\n","    targidx =\n","\n","    # TARGET get the activation for this token\n","    actsAll_trg[senti,:] =\n","\n","    # NON-TARGET get the activation for this token\n","    actsAll_non[senti,:] =\n","\n","  return actsAll_trg,actsAll_non"],"metadata":{"id":"wty47Z6V9V_k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nN5pA7je9Jmj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Gather statistics from each layers"],"metadata":{"id":"FIb-FRuK-e0Y"}},{"cell_type":"code","source":["# same bins for all histograms\n","edges = np.linspace(-1,1,101)\n","\n","# lots of initializations\n","yQQ_trg = np.zeros((model.config.n_layer,len(edges)-1))\n","yKK_trg = np.zeros((model.config.n_layer,len(edges)-1))\n","yVV_trg = np.zeros((model.config.n_layer,len(edges)-1))\n","yQQ_non = np.zeros((model.config.n_layer,len(edges)-1))\n","yKK_non = np.zeros((model.config.n_layer,len(edges)-1))\n","yVV_non = np.zeros((model.config.n_layer,len(edges)-1))\n","\n","yQK_trg = np.zeros((model.config.n_layer,len(edges)-1))\n","yQV_trg = np.zeros((model.config.n_layer,len(edges)-1))\n","yKV_trg = np.zeros((model.config.n_layer,len(edges)-1))\n","yQK_non = np.zeros((model.config.n_layer,len(edges)-1))\n","yQV_non = np.zeros((model.config.n_layer,len(edges)-1))\n","yKV_non = np.zeros((model.config.n_layer,len(edges)-1))\n","\n","all_vars = np.zeros((2,model.config.n_layer,3))\n","all_mean = np.zeros((2,model.config.n_layer,3))\n","\n","\n","\n","for layeri in range(model.config.n_layer):\n","\n","  ### get activations from this layer\n","  actsAll_trg,actsAll_non = get_QKV_activations(\n","\n","\n","  ### get variances\n","  vars = actsAll_trg.var(axis=0)\n","  all_vars[0,layeri,0] = np.mean(vars[:nEmb])\n","  all_vars[0,layeri,1] = np.mean(\n","  all_vars[0,layeri,2] = np.mean(\n","\n","  vars = actsAll_non.var(axis=0)\n","  all_vars[1,layeri,0] =\n","  all_vars[1,layeri,1] =\n","  all_vars[1,layeri,2] =\n","\n","\n","  ### get means\n","  meenz =\n","  all_mean[0,layeri,0] = np.mean(meenz[\n","  all_mean[0,layeri,1] = np.mean(meenz[\n","  all_mean[0,layeri,2] = np.mean\n","\n","  meenz = actsAll_non.var(axis=0)\n","\n","\n","\n","  ### calculate cosine similarity\n","  # TARGET\n","  actsAllNorm = actsAll_trg / np.linalg.norm()\n","  cossim_trg = actsAllNorm.T @\n","\n","  # NON-TARGET\n","  actsAllNorm =\n","  cossim_non =\n","\n","\n","\n","  ### Extract unique similarities\n","  # TARGET within\n","  QQcs_trg = cossim_trg[matrixMask==qLoc*qLoc]\n","  KKcs_trg = cossim_trg[\n","  VVcs_trg =\n","\n","  # cross-terms\n","  QKcs_trg = cossim_trg[matrixMask==qLoc*kLoc]\n","  QVcs_trg = cossim_trg[\n","  KVcs_trg =\n","\n","  # NON-TARGET within\n","  QQcs_non =\n","  KKcs_non =\n","  VVcs_non =\n","\n","  # cross-terms\n","  QKcs_non =\n","  QVcs_non =\n","  KVcs_non =\n","\n","\n","\n","  ### Generate histograms for target and non-target\n","\n","  ## TARGET\n","  # within-matrix histograms\n","  yQQ_trg[layeri,:],_ = np.histogram(QQcs_trg,bins=edges)\n","  yKK_trg[layeri,:],_ = np.histogram()\n","  yVV_trg[layeri,:],_ = np.histogram(\n","\n","  # and between-matrix\n","  yQK_trg[layeri,:],_ = np.histogram(QKcs_trg,bins=edges)\n","  yQV_trg[layeri,:],_ = np.\n","  yKV_trg[layeri,:],_ =\n","\n","\n","  ## NON-TARGET\n","  # within-matrix histograms\n","  yQQ_non[layeri,:],_ = np.histogram(QQcs_non,=edges)\n","  yKK_non[layeri,:],_ =\n","  yVV_non[layeri,:],_ =\n","\n","  # and between-matrix\n","  yQK_non[layeri,:],_ =\n","  yQV_non[layeri,:],_ =\n","  yKV_non[layeri,:],_ =\n"],"metadata":{"id":"VaMttXCB-exf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RUSnmSErnhVh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Visualize the cosine similarity histograms"],"metadata":{"id":"Ebzp33XinhSv"}},{"cell_type":"code","source":["fig,axs = plt.subplots(2,2,figsize=(12,6))\n","\n","\n","for layeri in range(model.config.n_layer):\n","\n","  #### plotting\n","  # QQ targets\n","  axs[0,0].plot(edges[:-1],,color=mpl.cm.plasma(layeri/model.config.n_layer))\n","  axs[0,0].set(xlim=edges[[0,-1]],xticks=[],yticks=[],ylabel='Count (a.u.)',title='Q-Q targets')\n","\n","\n","  # QQ nontargets\n","  axs[0,1].plot(edges[:-1],,color=mpl.cm.plasma(layeri/model.config.n_layer))\n","  axs[0,1].set(xlim=edges[[0,-1]],xticks=[],yticks=[],title='Q-Q nontargets')\n","\n","  # KV targets\n","  axs[1,0].plot(,color=mpl.cm.plasma(layeri/model.config.n_layer))\n","  axs[1,0].set(xlim=edges[[0,-1]],xlabel='Cosine similarity',ylabel='Count (a.u.)',yticks=[],title='K-V targets')\n","\n","  # KV nontargets\n","  axs[1,1].plot(\n","  axs[1,1].set(xlim=edges[[0,-1]],xlabel='Cosine similarity',yticks=[],title='K-V nontargets')\n","\n","\n","# manually adjust the y-lims\n","axs[0,0].set(ylim=[0,axs[0,0].get_ylim()[1]/4])\n","axs[1,0].set(ylim=[0,axs[1,0].get_ylim()[1]/4])\n","axs[0,1].set(ylim=[0,None])\n","axs[1,1].set(ylim=[0,None])\n","\n","# colorbar for line color (layer number)\n","cmap = mpl.colormaps['plasma']\n","norm = mpl.colors.BoundaryNorm(np.arange(model.config.n_layer), cmap.N)\n","sm = mpl.cm.ScalarMappable(cmap=cmap, norm=norm)\n","for a in axs.flatten(): cbar = fig.colorbar(sm, ax=a, pad=.01)\n","\n","\n","# finalize the plot\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"MzBnz2mmHgQ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-AOSuD7znlWm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,2,figsize=(12,3.5))\n","\n","h = axs[0].imshow(,origin='lower',aspect='auto',\n","              extent=[edges[0],edges[-1],0,model.config.n_layer])\n","axs[0].set(xlabel='Cosine similarity',ylabel='Layer',title='Q-Q target')\n","fig.colorbar(h,ax=axs[0],pad=.01,label='Count')\n","\n","\n","\n","axs[1].set(xlabel='Cosine similarity',ylabel='Layer',title='Q-Q nontarget')\n","fig.colorbar(h,ax=axs[1],pad=.01,label='Count')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"MxUfGfmBgL-Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zalXMMGneEMn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Visualize the means and variances"],"metadata":{"id":"A90f9PkQnrOe"}},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,4.5))\n","\n","for i in range(2):\n","  axs[i].plot(all_mean,'ko',markerfacecolor=[.9,.7,.7,.7],markersize=10,label='Q')\n","  axs[i].plot(,'ks',markerfacecolor=[.7,.9,.7,.7],markersize=10,label='K')\n","  axs[i].plot(,label='V')\n","  axs[i].set(xlabel='Layer',ylabel='Activation means',ylim=[all_mean.min()*1.5,all_mean.max()*1.1])\n","  axs[i].legend()\n","\n","axs[0].set(title='Targets')\n","axs[1].set(title='Non-targets')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"IkOcFjb2eEFN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vZATGJoLnmfw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,4.5))\n","\n","for i in range(2):\n","  axs[i].plot(label='Q')\n","  axs[i].plot(label='K')\n","  axs[i].plot(label='V')\n","  axs[i].set(xlabel='Layer',ylabel='Activation variance',ylim=[-.01,all_vars.max()*1.1])\n","  axs[i].legend()\n","\n","axs[0].set(title='Targets')\n","axs[1].set(title='Non-targets')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"U668_EbmeEH7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZDJPZOnJbz2T"},"execution_count":null,"outputs":[]}]}