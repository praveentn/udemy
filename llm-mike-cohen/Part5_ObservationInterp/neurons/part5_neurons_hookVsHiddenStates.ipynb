{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNTWeXVPq3uV5PpIJuTHaQP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating neurons and dimensions<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Relation between hooks and output.hidden_states<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"mlcJM5uEHZ6h"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RuKeB769HOkN"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","import torch\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","\n","model = AutoModelForCausalLM.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"]},{"cell_type":"code","source":["model"],"metadata":{"id":"9Xp3pAXhjvG1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jqTjLQ3NrqHj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Implant hooks for the attention sublayer activations"],"metadata":{"id":"YI_EkZGRHaXr"}},{"cell_type":"code","source":["# hook function to store attention vectors\n","activations = {}\n","\n","def implant_hook_attn(layer_number):\n","  def hook(module, input, output):\n","    activations[f'attn_proj_{layer_number}'] = output.detach()\n","  return hook\n","\n","# and mlp layers\n","def implant_hook_mlp(layer_number):\n","  def hook(module, input, output):\n","    activations[f'mlp_proj_{layer_number}'] = output.detach()\n","  return hook\n","\n","# implant hooks\n","layer2hook = 10\n","\n","model.transformer.h[layer2hook].attn.c_proj.register_forward_hook(implant_hook_attn(layer2hook))\n","model.transformer.h[layer2hook].mlp.c_proj.register_forward_hook(implant_hook_mlp(layer2hook))"],"metadata":{"id":"3kRaud4VHY6T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FH-Q2xkK49Sz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# \"Hook\" the activations"],"metadata":{"id":"fIfKI4MN8yvE"}},{"cell_type":"code","source":["text = \"Plants make fantastic pets because they don't leave a mess after you feed them.\"\n","tokens = tokenizer.encode(text,return_tensors='pt')\n","\n","# forward pass to trigger the hook\n","with torch.no_grad(): outputs = model(tokens,output_hidden_states=True)"],"metadata":{"id":"ksnGrkwiUFMo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hi_Dest47mWt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Reconstructing hidden_state from attn+mlp hooks"],"metadata":{"id":"ElAFtX8OG_D-"}},{"cell_type":"code","source":["print('Hidden state is size: ',outputs.hidden_states[layer2hook].shape)\n","print('Hooked MLP is size: ',activations[f'mlp_proj_{layer2hook}'].shape)"],"metadata":{"id":"inqDkDO3G9ki"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get the activations of the current and hidden states\n","hs_curr = outputs.hidden_states[layer2hook].detach()\n","hs_next = outputs.hidden_states[layer2hook+1].detach()\n","\n","# extract the attention and mlp deltas (updates to current embedding)\n","attn_delta = activations[f'attn_proj_{layer2hook}']\n","mlp_delta  = model.transformer.h[layer2hook].mlp.dropout(activations[f'mlp_proj_{layer2hook}'])\n","\n","\n","# create a threshold mask to remove extreme activation values (helps with the demo)\n","threshVal = 30\n","threshMask = np.full(hs_curr.shape,True)\n","for mat in [hs_curr,hs_next,attn_delta,mlp_delta]:\n","  threshMask[abs(mat)>threshVal] = False"],"metadata":{"id":"pOC2t0kWG9gY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reconstruct the next layer\n","reconstruction = hs_curr + attn_delta + mlp_delta"],"metadata":{"id":"D6Qdb2nEMJh3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# comparisons!\n","_,axs = plt.subplots(2,2,figsize=(10,8))\n","\n","\n","# correlate current and next layer hidden states\n","R = np.corrcoef(hs_curr[threshMask],hs_next[threshMask])[0,1]\n","axs[0,0].plot(hs_curr[threshMask],hs_next[threshMask],'ko',markerfacecolor=[.7,.7,.9,.6])\n","axs[0,0].set(xlabel=f'Layer {layer2hook}',ylabel=f'Layer {layer2hook+1}',title=f'r = {R:.3f}')\n","\n","\n","# correlate attention and MLP deltas\n","R = np.corrcoef(attn_delta[threshMask],mlp_delta[threshMask])[0,1]\n","axs[0,1].plot(attn_delta[threshMask],mlp_delta[threshMask],'ko',markerfacecolor=[.7,.9,.7,.6])\n","axs[0,1].set(xlabel='Attention $\\Delta$',ylabel='MLP $\\Delta$',title=f'r = {R:.3f}')\n","\n","\n","# correlate reconstruction and next layer\n","R = np.corrcoef(reconstruction[threshMask],hs_next[threshMask])[0,1]\n","axs[1,0].plot(reconstruction[threshMask],hs_next[threshMask],'ko',markerfacecolor=[.7,.9,.7,.6])\n","axs[1,0].set(xlabel=f'Layer {layer2hook} + $\\Delta$(att) + $\\Delta$(mlp)',ylabel=f'Layer {layer2hook+1}',title=f'r = {R:.3f}')\n","\n","\n","# correlate difference of layers with sum of deltas\n","layer_diff = (hs_next - hs_curr)[threshMask]\n","attention_sum = (attn_delta + mlp_delta)[threshMask]\n","R = np.corrcoef(layer_diff,attention_sum)[0,1]\n","axs[1,1].plot(layer_diff,attention_sum,'ko',markerfacecolor=[.9,.7,.7,.6])\n","axs[1,1].set(xlabel='Difference of layers',ylabel=r\"Sum of attention $\\Delta$'s\",title=f'r = {R:.3f}')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Pww5ZR-_JBnC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1TuMDKEuG9XD"},"execution_count":null,"outputs":[]}]}