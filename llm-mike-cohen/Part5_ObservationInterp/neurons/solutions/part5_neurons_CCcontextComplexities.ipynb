{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1dAVtJmyIKlaCJ7SjDy11zzZfM4NgZRpg","timestamp":1750504442023}],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyNRfhHVT88iID5lru+NxvhV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating neurons and dimensions<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Context-modulated activation in MLP<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"EBT02JwWkMri"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pel5HU1r9_0n"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","import requests\n","import textwrap\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch"]},{"cell_type":"code","source":[],"metadata":{"id":"KPPVI-Rp0NBc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Import the model and the nouns, implant the hook"],"metadata":{"id":"uu2yxPwQ0M-g"}},{"cell_type":"code","source":["# Eleuther's tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')\n","\n","# load in GPTneo's and push to GPU\n","model = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# move the model to the GPU\n","model = model.to(device)\n","model.eval();\n","\n","# number of MLP 'expansion' units\n","nneurons = model.transformer.h[8].mlp.c_fc.weight.shape[0]"],"metadata":{"id":"u1V-auBrPSS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# a hook function to grab the activations\n","activations = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # get the activations\n","    acts = module.c_fc(input[0])  # [batch, seq, 4xembed_dim]\n","\n","    # store in the dictionary\n","    activations[f'mlp_{layer_number}_x'] = acts\n","  return hook\n","\n","\n","# pick the layer to hook\n","layer2hook = 8\n","hookName = f'mlp_{layer2hook}_x'\n","\n","# surgery ;)\n","model.transformer.h[layer2hook].mlp.register_forward_hook(implant_hook(layer2hook))"],"metadata":{"id":"-5ZWugpPnt85"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import the nouns\n","url = 'https://raw.githubusercontent.com/david47k/top-english-wordlists/refs/heads/master/top_english_nouns_lower_10000.txt'\n","nouns = requests.get(url).text\n","nouns = nouns.split('\\n')[:100]"],"metadata":{"id":"D9LEmR85sH3W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QSS5mC2Jt6V9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Activations for words with and without spaces"],"metadata":{"id":"S65TJXA6n320"}},{"cell_type":"code","source":["# initialize tensor for all activations\n","all_activations = np.zeros((2,len(nouns),nneurons))\n","\n","\n","# loop over the tokens\n","for i,word in enumerate(nouns):\n","\n","  # forward pass one token without space\n","  with torch.no_grad(): model(tokenizer.encode(word,return_tensors='pt').to(device))\n","  all_activations[0,i,:] = activations[hookName].mean(dim=1).squeeze().detach().cpu().numpy()\n","\n","  # forward pass same token with preceeding space\n","  with torch.no_grad(): model(tokenizer.encode(f' {word}',return_tensors='pt').to(device))\n","  all_activations[1,i,:] = activations[hookName].mean(dim=1).squeeze().detach().cpu().numpy()\n"],"metadata":{"id":"JQ--6im_ypeA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convenience variables\n","noSpace  = all_activations[0,:,:].flatten()\n","yesSpace = all_activations[1,:,:].flatten()\n","\n","# square root of distances\n","diffs = np.sqrt(abs(noSpace-yesSpace))\n","diffs /= diffs.max()\n","\n","\n","# scatter plots\n","_,axs = plt.subplots(1,2,figsize=(12,5))\n","axs[0].scatter(noSpace,yesSpace,s=50,c=diffs,alpha=.5,cmap=mpl.cm.plasma_r)\n","axs[0].set(xlabel='No space',ylabel='With space',\n","              title=f'Correlation r = {np.corrcoef(noSpace,yesSpace)[0,1]:.3f}')\n","\n","\n","# histograms\n","y_yes,x_yes = np.histogram(yesSpace,bins=100,density=True)\n","y_noS,x_noS = np.histogram(noSpace,bins=100,density=True)\n","\n","axs[1].plot(x_yes[:-1],y_yes,linewidth=2,label='With space')\n","axs[1].plot(x_noS[:-1],y_noS,linewidth=2,label='No space')\n","\n","axs[1].legend()\n","axs[1].set(xlim=[-4,4],xlabel='Activation value',ylabel='Density',title='Distributions of MLP activations')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Q5b9nvYvdoRs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"51X0MhWt79tv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Get activations from generated tokens"],"metadata":{"id":"yw015XbD79l_"}},{"cell_type":"code","source":["# generate some new tokens\n","gentoks = model.generate(tokenizer.encode('I think the world could be better if',return_tensors='pt').to(device),\n","                         max_length=200, do_sample=True)\n","\n","# let's see what the model thinks :o\n","print(textwrap.fill(tokenizer.decode(gentoks[0]),60))"],"metadata":{"id":"5tYHqZFO79g1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad(): model(gentoks)\n","activations[hookName].shape"],"metadata":{"id":"ueaVMh3uDxhC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fulltext_activations = activations[hookName].cpu()"],"metadata":{"id":"n2726OlcRSgQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yVGTL0T679eL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Compare in-text and individual tokens"],"metadata":{"id":"ysslCD12b51Y"}},{"cell_type":"code","source":["allacts = np.zeros((2,nneurons,len(gentoks[0])))\n","\n","for ti,tok in enumerate(gentoks[0]):\n","\n","  # forward pass for just this token\n","  with torch.no_grad(): model(tok.unsqueeze(0).to(device))\n","\n","  # get the two activations\n","  allacts[0,:,ti] = fulltext_activations[0,ti,:].numpy()\n","  allacts[1,:,ti] = activations[hookName][0,ti,:].cpu().numpy()\n"],"metadata":{"id":"yHlFwddvUHcA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fromText = allacts[0,:,:].flatten()\n","fromToks = allacts[1,:,:].flatten()\n","\n","diffs = np.sqrt(abs(fromText-fromToks))\n","diffs /= diffs.max()\n","\n","\n","# scatter plots\n","_,axs = plt.subplots(1,2,figsize=(12,5))\n","axs[0].scatter(fromText,fromToks,s=50,c=diffs,alpha=.5,cmap=mpl.cm.plasma_r)\n","axs[0].set(xlabel='From text',ylabel='Individual tokens',\n","              title=f'Correlation r = {np.corrcoef(fromText,fromToks)[0,1]:.3f}')\n","\n","\n","# histograms\n","y_toks,x_toks = np.histogram(fromToks,bins=100,density=True)\n","y_text,x_text = np.histogram(fromText,bins=100,density=True)\n","y_diff,x_diff = np.histogram(fromToks-fromText,bins=100,density=True)\n","\n","axs[1].plot(x_toks[:-1],y_toks,linewidth=2,label='Individual tokens')\n","axs[1].plot(x_text[:-1],y_text,linewidth=2,label='From text')\n","axs[1].plot(x_diff[:-1],y_diff,linewidth=2,label='Difference')\n","\n","axs[1].legend()\n","axs[1].set(xlim=[-7,7],xlabel='Activation value',ylabel='Density',title='Distributions of MLP activations')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"0-ZsiW-3UHX6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Vz0kaygRYTi7"},"execution_count":null,"outputs":[]}]}