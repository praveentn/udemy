{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMGo0OWXYzpRq1dxyOivxId"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating neurons and dimensions<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: Activation histograms by token length<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"EBT02JwWkMri"}},{"cell_type":"code","source":["# run first to install and then restart\n","# !pip install -U datasets huggingface_hub fsspec"],"metadata":{"id":"p3a7ZCgN_YeN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pel5HU1r9_0n"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","from datasets import load_dataset\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":[],"metadata":{"id":"CDYgJm9whU-z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Import the model and implant hooks"],"metadata":{"id":"cj6kQvI7hU8A"}},{"cell_type":"code","source":["# for exercises 1-6\n","tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')\n","model = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n","\n","# for exercise 7\n","# tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-1.3B')\n","# model = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-1.3B')"],"metadata":{"id":"u1V-auBrPSS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# use GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# move the model to the GPU and switch to eval\n","model = model.to(device)\n","model.eval()"],"metadata":{"id":"pkljw5bmOyWJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hook function\n","activations = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # store in the dictionary\n","    activations[f'mlp_{layer_number}'] = module.c_fc(input[0]).cpu()\n","  return hook\n","\n","\n","# put hooks in all layers\n","for layeri in range(model.config.num_layers):\n","  model.transformer.h[layeri].mlp.register_forward_hook(implant_hook(layeri))"],"metadata":{"id":"-5ZWugpPnt85"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# number of MLP expansion neurons\n","nneurons = model.transformer.h[3].mlp.c_fc.weight.shape[0]"],"metadata":{"id":"DMvzP8wMhrd-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iiFw-dHeN0Rn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Import and tokenize fineweb"],"metadata":{"id":"uu2yxPwQ0M-g"}},{"cell_type":"code","source":["fineweb = load_dataset('HuggingFaceFW/fineweb', split='train', streaming=True)\n","fw_iterator = iter(fineweb)  # create iterator\n","\n","# get multiple examples:\n","for _ in range(5):\n","  example = next(fw_iterator)\n","  print('\\n',example['text'][:100])"],"metadata":{"id":"O6PU8Wp5LiSe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# how many tokens in total\n","desiredTokenCount = 8192\n","\n","# initialize empty tensor (must be ints!)\n","allTokens = torch.tensor([],dtype=torch.long)\n","allTokenLengths = np.array([])\n","\n","# reinitialize iterator\n","fw_iterator = iter(fineweb)\n","\n","\n","# keep importing data until we have enough\n","while allTokens.numel()<desiredTokenCount:\n","\n","  # import the text\n","  text = next(fw_iterator)['text']\n","\n","  # tokenize\n","  tokens = tokenizer.encode(text,return_tensors='pt')\n","\n","  # get token lengths\n","  tokenLengths = np.array([len(tokenizer.decode(t)) for t in tokens[0]])\n","\n","  # stack the tokens and the lengths\n","  allTokens = torch.cat( (allTokens,tokens) ,dim=-1)\n","  allTokenLengths = np.concatenate( (allTokenLengths,tokenLengths) )\n","\n","\n","# trim the vectors\n","allTokens = allTokens[0,:desiredTokenCount]\n","allTokenLengths = allTokenLengths[:desiredTokenCount]\n","\n","print(allTokens.shape)\n","print(allTokenLengths.shape)"],"metadata":{"id":"fZe2eq1I9QdC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# bar plot of token counts, with median\n","u,c = np.unique(allTokenLengths,return_counts=True)\n","medianTokLength = np.median(allTokenLengths)\n","\n","# make the bar graph\n","plt.figure(figsize=(10,4))\n","plt.bar(u,c,color=[.7,.7,.9],edgecolor='k')\n","plt.axvline(medianTokLength,linestyle='--',color='k',linewidth=3,label='Median')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Token character count',ylabel='Frequency',title='Distribution of token lengths')\n","plt.show()"],"metadata":{"id":"dYC66sYzk_bo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print a summary\n","print(f'There are {sum(allTokenLengths<medianTokLength):,} tokens shorter than the median.')\n","print(f'There are {sum(allTokenLengths>medianTokLength):,} tokens longer than the median.')\n","print(f'There are {sum(allTokenLengths==medianTokLength):,} tokens equal to the median.')"],"metadata":{"id":"xfO5bjU1g9sO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QSS5mC2Jt6V9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Get activations"],"metadata":{"id":"S65TJXA6n320"}},{"cell_type":"code","source":["# get a batch of tokens\n","print(allTokens.shape)\n","batch = allTokens.reshape(16,512)\n","batch.shape,type(batch)"],"metadata":{"id":"P3OBDpuF9pf4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# forward pass the batch\n","# ~1 min on cpu for 125m\n","# 2 secs on gpu for 1.3B (lol)\n","with torch.no_grad():\n","  model(batch.to(device))"],"metadata":{"id":"oYAzCoDTRj_K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["activations.keys()"],"metadata":{"id":"_bG4KHiGRpJ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check shape -- should be batch X tokens X nneurons\n","activations['mlp_10'].shape"],"metadata":{"id":"JQ--6im_ypeA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9Fg13-wav-Uf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Activations distributions by median split"],"metadata":{"id":"iYoGs0axtI1p"}},{"cell_type":"code","source":["# extract and flatten activations\n","acts = activations['mlp_4'].reshape(-1,nneurons)\n","\n","# activations by length split\n","binedges = torch.linspace(-8,5,51)\n","yS,_ = torch.histogram(acts[allTokenLengths<medianTokLength,:],bins=binedges,density=True)\n","yL,_ = torch.histogram(acts[allTokenLengths>medianTokLength,:],bins=binedges,density=True)\n","yM,_ = torch.histogram(acts[allTokenLengths==medianTokLength,:],bins=binedges,density=True)\n","\n","# visualize\n","plt.figure(figsize=(10,5))\n","plt.plot(binedges[:-1],yS,linewidth=2,label='Short tokens')\n","plt.plot(binedges[:-1],yL,linewidth=2,label='Long tokens')\n","plt.plot(binedges[:-1],yM,linewidth=2,label='Median tokens')\n","\n","plt.gca().set(xlim=binedges[[0,-1]],xlabel='Activations',ylabel='Density',\n","              title='Distribution of activations by token length')\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"ierevlqGtIyE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"j68J7qH1Ucfk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5: Activation-length correlations in one layer"],"metadata":{"id":"p-D8BmjvtIrS"}},{"cell_type":"code","source":["# get the activations and numpyify\n","acts = activations['mlp_4'].reshape(-1,nneurons).numpy()\n","\n","# standardize the activations from all neurons\n","zacts = (acts-acts.mean(axis=0,keepdims=True)) / np.std(acts,axis=0,ddof=1,keepdims=True)"],"metadata":{"id":"MF02laNjHDwx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confirm\n","zacts.shape, zacts[:,600].mean(), zacts[:,600].std(ddof=1)"],"metadata":{"id":"FXL5sx-FHDuH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# normalize the token lengths\n","zTokenLens = (allTokenLengths-allTokenLengths.mean()) / allTokenLengths.std(ddof=1)\n","\n","# confirm\n","zTokenLens.mean(), zTokenLens.std(ddof=1)"],"metadata":{"id":"kRJFfI_BibjM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confirm one correlation value\n","np.corrcoef(acts[:,0],allTokenLengths)"],"metadata":{"id":"sZb8TJytEtkh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# covariance of standardized variables\n","sum( zacts[:,0]*zTokenLens) / (desiredTokenCount-1)"],"metadata":{"id":"dEkZXE2PjqeP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate all correlation coefficients\n","allCorrs = np.zeros(nneurons)\n","\n","for ni in range(nneurons):\n","  allCorrs[ni] = sum(zTokenLens*zacts[:,ni]) / (desiredTokenCount-1)"],"metadata":{"id":"CTPxZadHEth4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# and visualize!\n","plt.figure(figsize=(8,4))\n","plt.hist(allCorrs,bins=100,color=[.7,.9,.7],linewidth=.5,edgecolor='gray')\n","\n","plt.gca().set(xlabel='Correlation coefficient',ylabel='Count',title='Histogram of all correlation coefficients')\n","plt.show()"],"metadata":{"id":"q21DB7W8Etcp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iy8db4sKHDzr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 6: Correlations in all layers"],"metadata":{"id":"Pg9E9eLmjesR"}},{"cell_type":"code","source":["allCorrs = np.zeros((model.config.num_layers,nneurons))\n","\n","# loop over all the layers\n","for layeri in range(model.config.num_layers):\n","\n","  # get and normalize the activations\n","  acts = activations[f'mlp_{layeri}'].reshape(-1,nneurons).numpy()\n","  zacts = (acts-acts.mean(axis=0,keepdims=True)) / np.std(acts,axis=0,ddof=1,keepdims=True)\n","\n","  # loop over all the neurons and correlate\n","  for ni in range(nneurons):\n","    allCorrs[layeri,ni] = sum(zTokenLens*zacts[:,ni]) / (desiredTokenCount-1)"],"metadata":{"id":"WV5MS2TXEtfO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# histograms\n","rEdges = torch.linspace(-.8,.8,81)\n","rHistCounts = np.zeros((model.config.num_layers,len(rEdges)-1))\n","\n","# get histogram of each layer\n","for layeri in range(model.config.num_layers):\n","  rHistCounts[layeri,:],_ = np.histogram(allCorrs[layeri,:],bins=rEdges,density=True)"],"metadata":{"id":"DQttY_pVHDrf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# and visualize\n","fig,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","for layeri in range(model.config.num_layers):\n","  axs[0].plot(rEdges[:-1],rHistCounts[layeri,:],color=mpl.cm.plasma(layeri/(model.config.num_layers-1)),label=f'MLP h.{layeri}')\n","\n","axs[0].legend()\n","axs[0].axvline(0,linestyle='--',color=[.7,.7,.7])\n","axs[0].set(xlabel='Correlation coefficients',ylabel='Density',xlim=rEdges[[0,-1]],\n","           title='Correlation histograms for each layer')\n","\n","# colorbar for line color (layer number)\n","cmap = mpl.colormaps['plasma']\n","norm = mpl.colors.BoundaryNorm(np.arange(model.config.num_layers), cmap.N)\n","sm = mpl.cm.ScalarMappable(cmap=cmap, norm=norm)\n","cbar = fig.colorbar(sm, ax=axs[0], pad=.01)\n","\n","\n","# image\n","h = axs[1].imshow(rHistCounts,aspect='auto',vmin=0,vmax=4,origin='lower',extent=[rEdges[0],rEdges[-1],0,model.config.num_layers])\n","axs[1].set(xlabel='Correlation coefficient',ylabel='Transformer block',title='Image of all histograms')\n","fig.colorbar(h,ax=axs[1],pad=.01)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"If3UT54CHDl6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"d7wu9xPBHDjR"},"execution_count":null,"outputs":[]}]}