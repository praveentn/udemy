{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1dAVtJmyIKlaCJ7SjDy11zzZfM4NgZRpg","timestamp":1750504442023}],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyOaOEvAYTdJnw7pk+cuFlqn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating neurons and dimensions<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Context-modulated activation in MLP<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"EBT02JwWkMri"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pel5HU1r9_0n"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","import requests\n","import textwrap\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch"]},{"cell_type":"code","source":[],"metadata":{"id":"KPPVI-Rp0NBc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Import the model and the nouns, implant the hook"],"metadata":{"id":"uu2yxPwQ0M-g"}},{"cell_type":"code","source":["# Eleuther's tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')\n","\n","# load in GPTneo's and push to GPU\n","model = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# move the model to the GPU\n","model = model.to(device)\n","model.eval();\n","\n","# number of MLP 'expansion' units\n","nneurons ="],"metadata":{"id":"u1V-auBrPSS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# a hook function to grab the activations\n","activations = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # get the activations\n","    acts =\n","\n","    # store in the dictionary\n","    activations[f'mlp_{layer_number}_x'] =\n","  return hook\n","\n","\n","# pick the layer to hook\n","layer2hook = 8\n","hookName =\n","\n","# surgery ;)\n","model.transformer.h[layer2hook].mlp."],"metadata":{"id":"-5ZWugpPnt85"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import the nouns\n","url = 'https://raw.githubusercontent.com/david47k/top-english-wordlists/refs/heads/master/top_english_nouns_lower_10000.txt'\n","nouns = requests.get(url).text\n","nouns ="],"metadata":{"id":"D9LEmR85sH3W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QSS5mC2Jt6V9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Activations for words with and without spaces"],"metadata":{"id":"S65TJXA6n320"}},{"cell_type":"code","source":["# initialize tensor for all activations\n","all_activations = np.zeros()\n","\n","\n","# loop over the tokens\n","for i,word\n","\n","  # forward pass one token without space\n","  with torch.no_grad(): model\n","  all_activations[0,i,:] = activations\n","\n","  # forward pass same token with preceeding space\n","  with torch.no_grad(): model(\n","  all_activations[1,i,:] = activations\n"],"metadata":{"id":"JQ--6im_ypeA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extract vectorized (flattened) variables\n","noSpace  =\n","yesSpace =\n","\n","# square root of distances\n","diffs =\n","diffs /= # normalize by max-val\n","\n","\n","# scatter plots\n","_,axs = plt.subplots(1,2,figsize=(12,5))\n","axs[0].scatter(,,s=50,c=diffs,alpha=.5,cmap=mpl.cm.plasma_r)\n","axs[0].set(xlabel='No space',ylabel='With space',\n","              title=f'Correlation r = {}')\n","\n","\n","# histograms\n","y_yes,x_yes = np.histogram(,bins=100,density=\n","y_noS,x_noS = np.histogram(,bins=100,density=\n","\n","axs[1].plot(x_yes[:-1],y_yes,linewidth=2,label='With space')\n","\n","\n","axs[1].legend()\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Q5b9nvYvdoRs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"51X0MhWt79tv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Get activations from generated tokens"],"metadata":{"id":"yw015XbD79l_"}},{"cell_type":"code","source":["# generate some new tokens\n","gentoks = model.generate(tokenizer.encode('I think the world could be better if',return_tensors='pt').to(device),\n","                         max_length=200, do_sample=True)\n","\n","# let's see what the model thinks :o\n","print("],"metadata":{"id":"5tYHqZFO79g1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad(): model(gentoks)\n","activations[hookName].shape"],"metadata":{"id":"ueaVMh3uDxhC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fulltext_activations = activations[hookName].cpu()"],"metadata":{"id":"n2726OlcRSgQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yVGTL0T679eL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Compare in-text and individual tokens"],"metadata":{"id":"ysslCD12b51Y"}},{"cell_type":"code","source":["allacts = np.zeros((2,nneurons,len(gentoks[0])))\n","\n","for ti,tok in\n","\n","  # forward pass for just this token\n","  with torch.no_grad(): model(tok.unsqueeze(0).to(device))\n","\n","  # get the two activations\n","  allacts[0,:,ti] = fulltext_activations\n","  allacts[1,:,ti] = activations\n"],"metadata":{"id":"yHlFwddvUHcA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# flattened activations again\n","fromText =\n","fromToks =\n","\n","# normalized difference magnitudes\n","diffs =\n","diffs /=\n","\n","\n","# scatter plots\n","_,axs = plt.subplots(1,2,figsize=(12,5))\n","\n","\n","# histograms\n","y_toks,x_toks = np.histogram\n","y_text,x_text = np.histogram\n","y_diff,x_diff = np.histogram\n","\n","axs[1].plot(label='Individual tokens')\n","axs[1].plot(label='From text')\n","axs[1].plot(label='Difference')\n","\n","axs[1].legend()\n","axs[1].set(xlim=[-7,7],xlabel='Activation value',ylabel='Density',title='Distributions of MLP activations')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"0-ZsiW-3UHX6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Vz0kaygRYTi7"},"execution_count":null,"outputs":[]}]}