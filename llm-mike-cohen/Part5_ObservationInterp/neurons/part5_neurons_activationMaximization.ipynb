{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyMo6pF6Kc1oqT2ThnhLvgWg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating neurons and dimensions<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Activation maximization (code implementation)<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"uZA5rrVINbO1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn.functional as F\n","from transformers import GPT2Model, GPT2Tokenizer\n","\n","# vector plots\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["# load GPT2 model and tokenizer\n","model = GPT2Model.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","# use GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# move the model to the GPU\n","model = model.to(device)\n","model.eval() # we're training the embeddings, not the model!\n","\n","\n","# a copy of the original embeddings\n","embeddings = model.wte.weight.detach().cpu()"],"metadata":{"id":"vEwYHoqWz0nB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TlHBlSYw2kyE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Initialize a random embeddings pattern"],"metadata":{"id":"rIbzBVCF2kvm"}},{"cell_type":"code","source":["# length of the token sequence\n","seq_len = 5\n","\n","# random embeddings with gradient tracking\n","optimized_embeddings = torch.randn((1, seq_len, embeddings.shape[1]), requires_grad=True, device=device)\n","\n","# normalize the std to that of the real embeddings matrix\n","torch.nn.init.normal_(optimized_embeddings, mean=0, std=torch.std(embeddings))\n","\n","# check the shape\n","optimized_embeddings.shape"],"metadata":{"id":"kLDp3Ffw2m1X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get the histogram values\n","ye,xe = np.histogram(embeddings.flatten(),bins=80)\n","yo,xo = np.histogram(optimized_embeddings.flatten().detach().cpu(),bins=80)\n","\n","plt.figure(figsize=(10,4))\n","plt.plot(xe[:-1],ye/np.max(ye),linewidth=2,label='Embeddings matrix')\n","plt.plot(xo[:-1],yo/np.max(yo),linewidth=2,label='Random matrix')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Weight values',ylabel='Frequency (max-norm)',xlim=xe[[0,-1]])\n","# plt.yscale('log') # optional, gives a better appreciation of the tails\n","plt.show()"],"metadata":{"id":"o1nsbiwF3dXv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"biob5FC9twKU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# How to input embeddings in the model"],"metadata":{"id":"oMHzIzbXtwFA"}},{"cell_type":"code","source":["# select a dimension to maximize\n","layer_idx = 8 # 8th transformer block with index 7\n","dim_idx = 91"],"metadata":{"id":"KPUDU4uety-G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# how to use the maximized embeddings\n","outputs = model(\n","    inputs_embeds = optimized_embeddings, # instead of input_ids\n","    output_hidden_states = True # request all activations exported\n","    )\n","\n","# the output\n","print(f'Size of outputs.hidden_states: {len(outputs.hidden_states)}')\n","print(f'e.g., size of activation from layer {layer_idx}: {outputs.hidden_states[layer_idx].shape}')"],"metadata":{"id":"4DY0-K4c9bnT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"an_64B6IzyQ5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Now for training"],"metadata":{"id":"p_dphViu6L-j"}},{"cell_type":"code","source":["n_steps = 500   # optimization steps\n","lr = .001       # learning rate\n","lambda_l2 = .01 # regularization amount\n","\n","# optimizer\n","optimizer = torch.optim.Adam([optimized_embeddings], lr=lr)"],"metadata":{"id":"BG2L6_0rzsWI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize vectors to store progress\n","activationVal = np.zeros(n_steps)\n","gradientNorm = np.zeros(n_steps)\n","\n","\n","# loop over training steps\n","for step in range(n_steps):\n","\n","  # clear gradient\n","  optimizer.zero_grad()\n","\n","  # patch embeddings directly into the model\n","  outputs = model(\n","      inputs_embeds = optimized_embeddings,\n","      output_hidden_states = True)\n","\n","  # extract the dimension's activation (averaged over tokens)\n","  allActivations = outputs.hidden_states[layer_idx]\n","  dim_activation = allActivations[0,:,dim_idx].mean()\n","\n","  # squared Euclidean distance for L2 normalization\n","  L2 = lambda_l2 * torch.sum(optimized_embeddings**2)\n","\n","  # minimize loss -> maximize activation\n","  loss = -dim_activation + L2\n","  activationVal[step] = dim_activation.item()\n","\n","\n","  # run gradient descent\n","  loss.backward()\n","\n","  # get the gradient norm\n","  gradientNorm[step] = optimized_embeddings.grad.norm().item()\n","\n","  # finish backprop\n","  optimizer.step()\n","\n","  if step%23==0:\n","    print(f'Step {step:4}/{n_steps}, Target activation: {activationVal[step]:6.2f} (vs. neighbor: {allActivations[0,:,dim_idx+1].mean():.2f})')"],"metadata":{"id":"ocWJVLUZz6P3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,3))\n","\n","# plot the activation magnitudes\n","axs[0].plot(activationVal,'o',markersize=4,markerfacecolor=[.7,.6,.9],markeredgecolor='none')\n","axs[0].set(xlabel='Training steps',ylabel='Dimension activation',title='\"Inverse loss\" optimization')\n","\n","# plot the gradient norms\n","axs[1].plot(gradientNorm,'o',markersize=4,markerfacecolor=[.9,.6,.7],markeredgecolor='none')\n","axs[1].set(xlabel='Training steps',ylabel='Embedding gradient norm',title='Norm of gradients')\n","\n","plt.show()"],"metadata":{"id":"SB5DgB8f7VwE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# redraw the histograms of embedding values\n","\n","# get the histogram values\n","yo2,xo2 = np.histogram(optimized_embeddings.flatten().detach().cpu(),bins=80)\n","\n","plt.figure(figsize=(10,4))\n","plt.plot(xe[:-1],ye/np.max(ye),linewidth=2,label='Embeddings matrix')\n","plt.plot(xo[:-1],yo/np.max(yo),linewidth=2,label='Random matrix')\n","plt.plot(xo2[:-1],yo2/np.max(yo2),linewidth=2,label='Optimized matrix')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Weight values',ylabel='Frequency (max-norm)',\n","              xlim=[-1,1])\n","plt.show()"],"metadata":{"id":"iekRfCOi7VtP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","plt.imshow(optimized_embeddings.squeeze().detach().cpu(),\n","           aspect='auto',vmin=-.3,vmax=.3,origin='lower')\n","\n","plt.gca().set(xlabel='Embedding dim.',ylabel='Token position')\n","plt.show()"],"metadata":{"id":"ariOrGfk8Lxj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bCn2piSK8Lu0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Find closest tokens"],"metadata":{"id":"LYtlUFOu9XvD"}},{"cell_type":"code","source":["# one embed\n","oneemb = optimized_embeddings[0][0].detach().cpu()\n","\n","# cosine similarity with all embedding vectors\n","cs = F.cosine_similarity(oneemb.unsqueeze(0), embeddings)\n","\n","# find the token with max cossim\n","maxtok = np.argmax(cs)\n","\n","# and visualize\n","plt.figure(figsize=(10,4))\n","plt.plot(cs,'ko',markerfacecolor=[.9,.7,.8,.6])\n","plt.gca().set(xlim=[-10,tokenizer.vocab_size+9],xlabel='Token index',ylabel='Cosine similarity',\n","              title=f'Similarities to all token embeddings (top token is \"{tokenizer.decode(maxtok)}\")')\n","plt.show()"],"metadata":{"id":"xa-WrtxcA4AM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# decode embeddings to closest tokens\n","optimized_tokens = []\n","\n","for emb in optimized_embeddings[0]:\n","\n","  # cosine similarity with embedding weights\n","  similarities = F.cosine_similarity(emb.unsqueeze(0).detach().cpu(), embeddings)\n","\n","  # find the max similarity\n","  maxtok = np.argmax(similarities)\n","  optimized_tokens.append(maxtok)\n","\n","print('Optimized token sequence:\\n',tokenizer.decode(optimized_tokens))"],"metadata":{"id":"-hV3-xvPz77c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dAz84yHT0foh"},"execution_count":null,"outputs":[]}]}