{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyM7cS/u2PnPbEIrp2TxNnOM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating token embeddings<h1>|\n","|<h2>Lecture:</h2>|<h1><b>Singular value spectrum of embedding submatrices<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"Jv6F0FKk1Xan"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTp8j3TJAqvB"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["# load BERT tokenizer and model\n","from transformers import BertTokenizer, BertModel\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","embeddings = model.embeddings.word_embeddings.weight.detach().numpy()"],"metadata":{"id":"IlLTVTpTBS75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FSbg2CS2vdiX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the figure in the beginning of the lecture\n","plt.figure(figsize=(6,5))\n","e1 = embeddings[tokenizer.encode('1',add_special_tokens=False)[0]]\n","e2 = embeddings[tokenizer.encode('2',add_special_tokens=False)[0]]\n","plt.plot(e1-e1.mean(),e2-e2.mean(),'ko',markerfacecolor=[.9,.7,.7,.6])\n","plt.gca().set(xlabel='Embedding 1',ylabel='Embedding 2')\n","plt.show()"],"metadata":{"id":"befekpDO_kpF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KXkqJxqs1aCf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Extract submatrices and mean-center"],"metadata":{"id":"Ya08LEFcpAfY"}},{"cell_type":"code","source":["# get the tokens for lower-case letters\n","import string\n","lettertoks = [tokenizer.encode(i,add_special_tokens=False)[0] for i in string.ascii_lowercase]\n","\n","# and the same number of random tokens\n","randtoks = np.random.randint(0,high=tokenizer.vocab_size,size=len(lettertoks))"],"metadata":{"id":"1_86AlDZt0hO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create embeddings submatrices for letters and randomly selected tokens\n","subembLetters = embeddings[lettertoks,:]\n","subembRandom = embeddings[randtoks,:]\n","\n","# mean-center\n","lettersMeanVect = subembLetters.mean(axis=0)[None,:]\n","randomMeanVect  = subembRandom.mean(axis=0)[None,:]\n","\n","subembLetters -= lettersMeanVect\n","subembRandom  -= randomMeanVect"],"metadata":{"id":"hAVJdD081Z_D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"t9aqvhu9pDST"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# SVD"],"metadata":{"id":"QXd8sBB4pDPv"}},{"cell_type":"code","source":["# SVD of both matrices (don't need the U matrices, so those can overwrite)\n","U,sRandom,VtRandom = np.linalg.svd(subembRandom)\n","U,sLetters,VtLetters = np.linalg.svd(subembLetters)\n","\n","# print sizes\n","print(f'Embeddings is size {subembRandom.shape}')\n","print(f'U  is size {U.shape}')\n","print(f's  is size {sLetters.shape}')\n","print(f'Vh is size {VtLetters.shape}')"],"metadata":{"id":"s6RXr9c8ovws"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GGhI3o326PTD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualize the spectra"],"metadata":{"id":"urY-iK4Ho_PH"}},{"cell_type":"code","source":["fig = plt.figure(figsize=(12,5))\n","gs = GridSpec(2,2,figure=fig)\n","\n","ax0 = fig.add_subplot(gs[0,0])\n","ax1 = fig.add_subplot(gs[1,0])\n","ax2 = fig.add_subplot(gs[:,1])\n","\n","ax0.plot(VtLetters[0],'k')\n","ax0.set(xlim=[0,model.config.hidden_size],xticklabels=[],ylabel='Score',\n","        title='Top letters singular vector')\n","\n","ax1.plot(VtRandom[0],'b')\n","ax1.set(xlim=[0,model.config.hidden_size],xticklabels=[],xlabel='Embedding dimension',ylabel='Score',\n","        title='Top random-token singular vector')\n","\n","\n","# plot their spectra\n","ax2.plot(sLetters,'ks-',markerfacecolor=[.7,.7,.9],label='Letters',markersize=8)\n","ax2.plot(sRandom,'bo-',markerfacecolor=[.9,.7,.7],label='Random',markersize=8)\n","ax2.set(xlabel='Component (sorted index)',ylabel='Singular value (max-norm)',title='Spectrum of embeddings submatrix')\n","ax2.legend()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"qMCuTfRU1Z70"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"j4CGv6ZJ6XPU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Project all embeddings onto the eigenvector"],"metadata":{"id":"Utid_sOusHbR"}},{"cell_type":"code","source":["# mean-center\n","embeddingsCentered = embeddings - lettersMeanVect\n","\n","# project all embeddings onto the singular vector\n","projections = embeddingsCentered @ VtLetters[0,:]\n","\n","plt.figure(figsize=(12,4))\n","plt.plot(projections,'ko',markerfacecolor=[.7,.7,.7,.6])\n","plt.gca().set(xlabel='Token',ylabel='Projection')\n","plt.show()"],"metadata":{"id":"x2JOqqYosLhu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print top magnitude-projection tokens\n","sortidx = np.argsort(abs(projections))[::-1]\n","\n","# print the top positive projections\n","for i in range(20):\n","  token = tokenizer.decode(sortidx[i])\n","  if token not in string.ascii_letters:\n","    print(f'{projections[sortidx[i]]:6.3f} for \"{token}\"')"],"metadata":{"id":"PwEINN9FsHYx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6Wv8aKSRGGnl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# correlation matrix of all letter embeddings\n","plt.imshow(abs(np.corrcoef(subembLetters)),vmin=0,vmax=1)\n","\n","plt.gca().set(xlabel='Tokens',ylabel='Tokens',title='|R| for letter embeddings')\n","plt.colorbar()\n","plt.show()"],"metadata":{"id":"895Vsv4MsHTO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"S2wiukqGGFjU"},"execution_count":null,"outputs":[]}]}