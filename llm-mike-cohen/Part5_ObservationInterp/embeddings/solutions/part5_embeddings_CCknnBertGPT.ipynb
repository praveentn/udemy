{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNVovrCBvLo0FM+NPK31y95"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating token embeddings<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: BERT v GPT kNN kompetition<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"C3tvDUkVtF80"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTp8j3TJAqvB"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["# load BERT tokenizer and model\n","from transformers import BertTokenizer, BertModel\n","\n","model = BertModel.from_pretrained('bert-base-uncased')\n","tokenizerB = BertTokenizer.from_pretrained('bert-base-uncased')\n","embeddingsB = model.embeddings.word_embeddings.weight.detach().numpy()"],"metadata":{"id":"IlLTVTpTBS75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pretrained GPT-2 model and tokenizer\n","from transformers import GPT2Model,GPT2Tokenizer\n","model = GPT2Model.from_pretrained('gpt2')\n","tokenizerG = GPT2Tokenizer.from_pretrained('gpt2')\n","embeddingsG = model.wte.weight.detach().numpy()"],"metadata":{"id":"Tn9guU3EWzMz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sj-DN5Owb_ed"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: kNN-based synonym-searching of \"ring\""],"metadata":{"id":"IYa6IRn8NZQK"}},{"cell_type":"code","source":["# define a normalized \"seed\" vector\n","seedword = 'ring'\n","\n","# check in BERT\n","seedidxB = tokenizerB.encode(seedword,add_special_tokens=False)\n","\n","# check in GPT\n","seedidxG = tokenizerG.encode(seedword)\n","\n","print(f'In BERT: \"{seedword}\" has index {seedidxB}')\n","print(f'In GPT2: \"{seedword}\" has index {seedidxG}')"],"metadata":{"id":"j5Mq2VmS9Q5s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Euclidean distances\n","eucDist_bert = np.sqrt(np.sum( (embeddingsB-embeddingsB[seedidxB,:])**2 ,axis=1))\n","eucDist_gpt2 = np.sqrt(np.sum( (embeddingsG-embeddingsG[seedidxG,:])**2 ,axis=1))\n","\n","# visualize the distributions\n","plt.figure(figsize=(10,4))\n","yB,xB = np.histogram(eucDist_bert,bins=90,density=True)\n","yG,xG = np.histogram(eucDist_gpt2,bins=90,density=True)\n","\n","plt.plot(xG[:-1],yG,linewidth=2,label='GPT2')\n","plt.plot(xB[:-1],yB,linewidth=2,label='BERT')\n","plt.legend()\n","plt.gca().set(xlim=[min(xB.min(),xG.min()),max(xB.max(),xG.max())],\n","              xlabel='Distances',ylabel='Density',title=f'Distances from \"{seedword}\"')\n","plt.show()"],"metadata":{"id":"38dhFl7YNc8G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sort and get top k\n","k = 15\n","topKidx_Bert = np.argsort(eucDist_bert)[:k] # [1:k+1] to exclude trivial self-distance\n","topKidx_gpt2 = np.argsort(eucDist_gpt2)[:k]\n","\n","# and print\n","print('|        BERT        |        GPT2          |')\n","print('|--------------------|----------------------|')\n","for b,g in zip(topKidx_Bert,topKidx_gpt2):\n","  print(f'  {tokenizerB.decode([b]):>10} ({eucDist_bert[b]:.2f})  |  ({eucDist_gpt2[g]:4.2f}) {tokenizerG.decode([g])}')"],"metadata":{"id":"f7YhIKAmNZNP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"31LgVYBy6fBx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Using normalized vectors"],"metadata":{"id":"Y1We4K82-rQj"}},{"cell_type":"code","source":["# normalize\n","embeddingsBnorm = embeddingsB / np.linalg.norm(embeddingsB, axis=1, keepdims=True)\n","embeddingsGnorm = embeddingsG / np.linalg.norm(embeddingsG, axis=1, keepdims=True)\n","\n","# Euclidean distances\n","eucDist_bert = np.sqrt(np.sum( (embeddingsBnorm-embeddingsBnorm[seedidxB,:])**2 ,axis=1))\n","eucDist_gpt2 = np.sqrt(np.sum( (embeddingsGnorm-embeddingsGnorm[seedidxG,:])**2 ,axis=1))\n","\n","\n","# visualize the distributions\n","plt.figure(figsize=(10,4))\n","yB,xB = np.histogram(eucDist_bert[np.nonzero(eucDist_bert)],bins=90,density=True)\n","yG,xG = np.histogram(eucDist_gpt2[np.nonzero(eucDist_gpt2)],bins=90,density=True)\n","\n","plt.plot(xG[:-1],yG,linewidth=2,label='GPT2')\n","plt.plot(xB[:-1],yB,linewidth=2,label='BERT')\n","plt.legend()\n","plt.gca().set(xlim=[min(xB.min(),xG.min()),max(xB.max(),xG.max())],\n","              xlabel='Distances',ylabel='Density',title=f'Distances from \"{seedword}\" with vector normalization')\n","plt.show()"],"metadata":{"id":"I1rjjUu6_aW_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sort and get top k\n","k = 15\n","topKidx_Bert = np.argsort(eucDist_bert)[:k] # [1:k+1] to exclude trivial self-distance\n","topKidx_gpt2 = np.argsort(eucDist_gpt2)[:k]\n","\n","# and print\n","print('|        BERT        |        GPT2          |')\n","print('|--------------------|----------------------|')\n","for b,g in zip(topKidx_Bert,topKidx_gpt2):\n","  print(f'  {tokenizerB.decode([b]):>10} ({eucDist_bert[b]:.2f})  |  ({eucDist_gpt2[g]:.2f}) {tokenizerG.decode([g])}')"],"metadata":{"id":"_bplpRXL-rM-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zbnBxp_B-rEy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: The importance of emptiness"],"metadata":{"id":"vTEQfPdlCZMZ"}},{"cell_type":"code","source":["# no new code here :P"],"metadata":{"id":"PNMuLY2gCb7Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9qXzL5r-CZDp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4: Multitoken words in GPT2"],"metadata":{"id":"NvBcZOJrwK_k"}},{"cell_type":"code","source":["# define a normalized \"seed\" vector\n","seedword = 'beauty'\n","\n","# check in BERT\n","seedidxB = tokenizerB.encode(seedword,add_special_tokens=False)\n","\n","# check in GPT\n","seedidxG = tokenizerG.encode(seedword)\n","\n","print(f'In BERT: \"{seedword}\" has index {seedidxB}')\n","print(f'In GPT2: \"{seedword}\" has index {seedidxG}')"],"metadata":{"id":"ToZUTBKUwO70"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Euclidean distances\n","eucDist_bert = np.sqrt(np.sum( (embeddingsB-embeddingsB[seedidxB,:].mean(axis=0))**2 ,axis=1))\n","eucDist_gpt2 = np.sqrt(np.sum( (embeddingsG-embeddingsG[seedidxG,:].mean(axis=0))**2 ,axis=1))\n","\n","# sort and get top k\n","k = 15\n","topKidx_Bert = np.argsort(eucDist_bert)[:k] # [1:k+1] to exclude trivial self-distance\n","topKidx_gpt2 = np.argsort(eucDist_gpt2)[:k]\n","\n","# and print\n","print('|        BERT        |        GPT2          |')\n","print('|--------------------|----------------------|')\n","for b,g in zip(topKidx_Bert,topKidx_gpt2):\n","  print(f'  {tokenizerB.decode([b]):>10} ({eucDist_bert[b]:.2f})  |  ({eucDist_gpt2[g]:4.2f}) {tokenizerG.decode([g])}')"],"metadata":{"id":"Xg13M_nIwKtC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"o6XJHybuAjY7"},"execution_count":null,"outputs":[]}]}