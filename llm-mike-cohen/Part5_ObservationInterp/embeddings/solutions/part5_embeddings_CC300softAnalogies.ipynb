{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyORXny5fPLND+PTLyrqIHzs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n","|-|:-:|\n","|<h2>Part 5:</h2>|<h1>Observation (non-causal) mech interp<h1>|\n","|<h2>Section:</h2>|<h1>Investigating token embeddings<h1>|\n","|<h2>Lecture:</h2>|<h1><b>CodeChallenge: soft-coded analogies in word2vec<b></h1>|\n","\n","<br>\n","\n","<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n","<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n","<i>Using the code without the course may lead to confusion or errors.</i>"],"metadata":{"id":"sIlUpZ6es2Vm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ofnv1L3ygQOj"},"outputs":[],"source":["# !pip install gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3lh-aPx04n2Y"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import gensim.downloader as api\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"]},{"cell_type":"code","source":["# import word2vec\n","w2v = api.load('word2vec-google-news-300')"],"metadata":{"id":"qos7Eh4J4rT6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PRJknz7Cioqx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1: Create the analogy vectors"],"metadata":{"id":"EQgiZntofjP7"}},{"cell_type":"code","source":["# get a list of all keys\n","allwords = list(w2v.key_to_index.keys())"],"metadata":{"id":"OHgfKF2IipQR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# soft-code the three words\n","word2start = 'king'\n","word2subtract = 'man'\n","word2add = 'woman'\n","\n","# test that all three words are in the vocabulary\n","for w in [word2start,word2subtract,word2add]:\n","  if w not in allwords:\n","    print(f'WARNING: \"{w}\" is not in word2vec!')\n","\n","# print the analogy\n","print(f'\\n\"{word2start}\" is to \"{word2subtract}\" as \"_____\" is to \"{word2add}\"')"],"metadata":{"id":"Id5ig0bWgknM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get the embeddings vectors\n","v1 = w2v[word2start]    # base word\n","v2 = w2v[word2subtract] # to subtract\n","v3 = w2v[word2add]      # to add\n","\n","# analogy vector\n","analogyVector = v1 - v2 + v3\n","\n","# plot the vectors\n","plt.figure(figsize=(10,4))\n","plt.plot(v1,label=word2start)\n","plt.plot(v2,label=word2subtract)\n","plt.plot(v3,label=word2add)\n","plt.plot(analogyVector,'k',linewidth=2,label='analogy')\n","\n","plt.gca().set(xlim=[0,len(v1)],xlabel='Embedding dimension',ylabel='Value')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"vRCCMmzmBPpx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"W5fsiIl3fmJP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2: Cosine similarity to all vectors"],"metadata":{"id":"W8iIEP3afmGs"}},{"cell_type":"code","source":["# cossim with all\n","cossim2all = w2v.cosine_similarities(analogyVector,w2v.vectors)\n","print(cossim2all.shape)\n","\n","# plot the cosine similarities (skip every N)\n","skip4plotting = 1000\n","_,axs = plt.subplots(1,2,figsize=(13,4))\n","axs[0].scatter(range(0,len(cossim2all),skip4plotting),cossim2all[::skip4plotting],c=cossim2all[::skip4plotting],marker='o',alpha=.5,cmap='seismic')\n","axs[0].set(xticks=[],xlabel=f'Index (skip every {skip4plotting})',ylabel='Cosine similarity',title='Similarity with analogy vector')\n","\n","axs[1].plot(np.sort(cossim2all)[-1000:],'ko',markerfacecolor='gray',alpha=.5)\n","axs[1].set(xticks=[],xlabel='Sorted index',ylabel='Cosine similarity',title='Top 1000 similarities')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"55OJXSdEUI6Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find top 10 highest scores\n","top10 = cossim2all.argsort()[-10:][::-1]\n","\n","# print them out\n","print('Top 10 closest words:')\n","for widx in top10:\n","  print(f'  Similarity of {cossim2all[widx]:.3f} with \"{w2v.index_to_key[widx]}\"')"],"metadata":{"id":"yeTg7gi7VMRN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KOCv6VFZjFp_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 3: Create and test an analogy function"],"metadata":{"id":"i42YpClIjujq"}},{"cell_type":"code","source":["def analogyCalculator(word2start,word2subtract,word2add):\n","\n","  # give error if a word is not in the vocabulary\n","  for w in [word2start,word2subtract,word2add]:\n","    if w not in allwords:\n","      raise ValueError(f'Error: \"{w}\" is not in word2vec!')\n","\n","  # print the analogy\n","  print(f'\\n\"{word2start}\" is to \"{word2subtract}\" as \"_____\" is to \"{word2add}\"')\n","\n","  # get the vectors\n","  v1 = w2v[word2start]    # base word\n","  v2 = w2v[word2subtract] # to subtract\n","  v3 = w2v[word2add]      # to add\n","\n","  # analogy vector\n","  analogyVector = v1 - v2 + v3\n","\n","  # cossim with all\n","  cossim2all = w2v.cosine_similarities(analogyVector,w2v.vectors)\n","\n","  # print out the top 10 highest scores\n","  top10 = cossim2all.argsort()[-10:][::-1]\n","  print('\\nTop 10 closest words:')\n","  for widx in top10:\n","    print(f'  Similarity of {cossim2all[widx]:.3f} with \"{w2v.index_to_key[widx]}\"')"],"metadata":{"id":"WPBS1KsajyJV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check accuracy of function\n","analogyCalculator('king','man','woman')"],"metadata":{"id":"epJA1WYFjyMF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check error handling\n","analogyCalculator('king','man','woman0')"],"metadata":{"id":"xlQzhCHyqOfM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### try some other analogies\n","\n","analogyCalculator('tree','leaf','petal')\n","# analogyCalculator('leaf','tree','flower')    # turn it around for better results?\n","# analogyCalculator('dachshund','dog','bird')\n","# analogyCalculator('tired','yawn','scratch')   # expecting \"itch\"\n","# analogyCalculator('finger','hand','foot')"],"metadata":{"id":"OvGc8-0WjyOl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"T9UgQlNzljaa"},"execution_count":null,"outputs":[]}]}